[
  {
    "objectID": "Teaching.html",
    "href": "Teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Math 6A, Summer Session A 2021 (Remote Instruction)"
  },
  {
    "objectID": "Teaching.html#courses-as-instructor-of-record",
    "href": "Teaching.html#courses-as-instructor-of-record",
    "title": "Teaching",
    "section": "",
    "text": "Math 6A, Summer Session A 2021 (Remote Instruction)"
  },
  {
    "objectID": "Teaching.html#courses-as-a-teaching-assistant",
    "href": "Teaching.html#courses-as-a-teaching-assistant",
    "title": "Teaching",
    "section": "Courses as a Teaching Assistant",
    "text": "Courses as a Teaching Assistant\n\nMath 117, Summer Session A 2024\nMath 118C, Spring 2024\nMath 118B, Winter 2024\nMath 6A, Winter 2022\nMath 6B, Fall 2021\nMath 117, Summer Session B 2021 (Remote Instruction)\nMath 4B, Spring 2021 (Remote Instruction)\nMath 118B, Winter 2021 (Remote Instruction)\nMath 118A, Fall 2020 (Remote Instruction)\nMath 6B, Summer Session B 2020 (Remote Instruction)\nMath 3A, Spring 2020 (Remote Instruction)\nMath 6B, Winter 2020\nMath 34A, Fall 2019"
  },
  {
    "objectID": "Research.html",
    "href": "Research.html",
    "title": "Research",
    "section": "",
    "text": "Here’s my CV (Updated July 2023)\nI’m working with the applied mathematics group at UCSB, under the supervision of Carlos Garcia-Cervera. My general areas of research are:\n\nRelativistic Quantum Mechanics\nVariational Problems Pertaining to Energy Minimization of Quantum Systems\nFunctional Analysis\nPartial Differential Equations\nNumerical Methods in Quantum Mathematics\n\nI’ll post publications/preprints here when they become available."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "I am a mathematics Ph.D. candidate at the University of California, Santa Barbara.\nMy office is in South Hall, Room 6432K (in the grad tower).\nI maintain my blog on this site, as well as post research updates.\nemail"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "Blog/Fractional Derivatives.html",
    "href": "Blog/Fractional Derivatives.html",
    "title": "Fractional Derivatives",
    "section": "",
    "text": "This is meant to be a very quick exposition on fractional derivatives, with particular emphasis paid to functions in \\(H^{1/2}\\). When I introduce my area of research to colleagues, I inevitably discuss four-spinors \\(\\psi\\in H^{1/2}(\\mathbb{R}^3;\\mathbb{C}^4)\\). This usually leads to an impromptu discussion of \\(H^{1/2}\\) and why we care about it. I’ll attempt to give a brief overview of these ideas here."
  },
  {
    "objectID": "Blog/Fractional Derivatives.html#the-sobolev-space-hs",
    "href": "Blog/Fractional Derivatives.html#the-sobolev-space-hs",
    "title": "Fractional Derivatives",
    "section": "The Sobolev Space \\(H^s\\)",
    "text": "The Sobolev Space \\(H^s\\)\nRecall that Sobolev spaces comprise of functions who’s derivative(s) exist in some weak sense. One of the purposes of considering such functions is to solve a PDE over a larger class of functions than the PDE’s domain. One can then try to recover appropriate regularity of the “weak” solution of the PDE. In this sense, one defines functions with \\(k\\) weak derivatives for some positive integer \\(k\\). It turns out you can generalize this notation to define fractional derivatives of functions. We reference (Evans 2010) and (Lieb and Loss 2001) for this discussion.\nThe space \\(H^s(\\mathbb{R}^n)\\) for any positive real number \\(s\\) consists of functions \\(f\\in L^2(\\mathbb{R}^n)\\) with the property that \\((1+|x|^s)\\widehat{f}\\in L^2(\\mathbb{R}^n)\\). The associated norm is,\n\\[||f||_{H^s(\\mathbb{R}^n)}:=||(1+|x|^s)\\widehat{f}||_2\\]\nIn the particular case of \\(H^{1/2}\\) this is sometimes equivalently characterized as,\n\\[||f||_{H^{1/2}(\\mathbb{R}^n)}^2:=||(1+|x|^2)^{1/2}|\\widehat{f}(x)|^2||_2^2\\]\nThis Fourier characterization of Sobolev spaces for integer \\(s\\) is equivalent to the more traditional definition of weakly differentiable functions. It can be very useful for both analytical as well as technical reasons. First, let me give a simple example of the former. Using the fourier characterization, one easily shows that if \\(f\\in H^s(\\mathbb{R}^n)\\) where \\(s&gt;\\frac{n}{2}\\), then in fact \\(f\\in L^\\infty(\\mathbb{R}^n)\\). This follows from the following computation,\n\\[|f(x)|=\\left|\\frac{1}{(2\\pi)^{n/2}}\\int_{\\mathbb{R}^n}e^{-ip\\cdot x}\\widehat{f}(p)dx\\right|\\leq\\frac{1}{(2\\pi)^{n/2}}\\int\\frac{1+|p|^s}{1+|p|^s}|\\widehat{f}(p)|dx \\]\n\\[\\leq\\frac{1}{(2\\pi)^{n/2}}||f||_{H^s(\\mathbb{R}^n)}^2\\left|\\left|\\frac{1}{1+|p|^s}\\right|\\right|_2^2\\]\nwhere the latter inequality follows by Holder’s inequality. Since \\(s&gt;\\frac{n}{2}\\), the right hand side is finite, as needed."
  },
  {
    "objectID": "Blog/Fractional Derivatives.html#fractional-differential-operators",
    "href": "Blog/Fractional Derivatives.html#fractional-differential-operators",
    "title": "Fractional Derivatives",
    "section": "Fractional Differential Operators",
    "text": "Fractional Differential Operators\nThe fractional Sobolev space \\(H^{1/2}\\) arises naturally in relativistic quantum mechanics (Thaller 2011). The Klein-Gordon operator arises from quantizing the classical relativistic energy-momentum equation. The operator is,\n\\[\\sqrt{m^2c^4-c^2\\Delta}\\tag{1}\\]\nHow do we apply such an operator? Such fractional operators are defined using the Fourier transform.\n\\[\\sqrt{m^2c^4-c^2\\Delta}f(x):=\\left(\\sqrt{m^2c^4+c^2p^2}\\widehat{f}(p)\\right)^{\\vee}\\tag{2}\\]\nWhen solving variational problems in quantum mathematics, we study the energy of quantum systems and try to find the ground state of a quantum system. Alternatively, we seek critical points of quantum energy functionals. The operator (1) isn’t really what we analyze when studying a variational problem. In fact, we study the energy,\n\\[(f(x),\\sqrt{m^2c^4-c^2\\Delta}f(x))\\tag{3}\\]\nwhich makes sense when \\(f\\) is only in \\(H^{1/2}\\), whereas operating on \\(f\\) as in (2) requires \\(f\\in H^1\\) (since (2) needs to be in \\(L^2\\)). This amounts to a weakening of the weak differentiability requirements of a function we input into the energy (3). We say \\(f\\) is in the “form domain” of the Klein-Gordon operator."
  },
  {
    "objectID": "Blog/Fractional Derivatives.html#application-to-dirac",
    "href": "Blog/Fractional Derivatives.html#application-to-dirac",
    "title": "Fractional Derivatives",
    "section": "Application to Dirac",
    "text": "Application to Dirac\nThe purpose of this post is not to give a complete exposition of the free Dirac operator; this is just a quick application of fractional operators to the operator I study every day. Let \\(D\\) denote the free Dirac operator with normalized units,\n\\[D_c:=-i\\pmb{\\alpha}\\cdot\\nabla+\\beta\\]\nwhere the three-vector \\(\\pmb{\\alpha}\\) has components \\(\\begin{pmatrix}0&\\pmb{\\sigma}_j\\\\\\pmb{\\sigma}_j&0\\end{pmatrix}\\), \\(\\pmb{\\sigma}_j\\) being the usual Pauli matrices. \\(\\beta:=\\begin{pmatrix}1&0\\\\0&-1\\end{pmatrix}\\). Thus, \\(D\\) operators on 4-spinors.\nThe Dirac operator, under the Foldy-Wouthuysen Transformation, takes the form,\n\\[\\begin{pmatrix}\\sqrt{1-\\Delta}&&0\\\\0&&-\\sqrt{1-\\Delta}\\end{pmatrix}\\tag{4}\\]\nThis makes it easy to see that the form domain of the Dirac operator is \\(H^{1/2}(\\mathbb{R}^3;\\mathbb{C}^4)\\). The Klein-Gordon equations arise in (4) from the fact that \\(|D|=\\sqrt{1-\\Delta}\\). In fact, if we consider the energy of a particle under the influence of Dirac, we may write,\n\\[(\\psi,H\\psi)_2=(\\psi^+,H\\psi^+)_2+(\\psi^-,H\\psi^-)_2\\]\nHere, \\(\\psi=\\psi^++\\psi^-\\) where \\(\\psi^+\\) lies in the positive spectral subspace induced by \\(D\\) and \\(\\psi^-\\) lies in the negative spectral subspace. Since \\((\\psi^+,H\\psi^+)=(\\psi^+,|H|\\psi^+)\\) and \\((\\psi^-,H\\psi^-)=-(\\psi^-,|H|\\psi^-)\\), we have,\n\\[(\\psi,H\\psi)_2=||\\psi^+||_{H^{1/2}}^2-||\\psi^-||_{H^{1/2}}^2\\]\nso that the energy may be written entirely in terms of the \\(H^{1/2}\\) norm of the positive and negative spectral components of \\(\\psi\\).\nIn the analysis of energy functionals based on Dirac (say, the Dirac-Fock energy), it turns out that the best way to prove the existence of critical points is to search for weak critical points; that is, the critical points are in the form domain \\(H^{1/2}\\) (Maria J. Esteban and Séré 1999). Afterwards, one may recover some regularity of the solutions and, in fact, the self-adjointness of the Dirac operator perturbed by the Coulomb may be used to prove that the solutions are in fact in \\(H^1\\) (M. J. Esteban and Sere 2001)."
  },
  {
    "objectID": "Blog/Neural Poisson Solver.html",
    "href": "Blog/Neural Poisson Solver.html",
    "title": "Neural Poisson Solver",
    "section": "",
    "text": "This is the first of a series of posts discussing a side project I’m working on. Long term, I’m interested in solving the PDEs I’m studying in my research using deep neural networks. This is a natural approach, but having not implemented such a program before, I’m starting with some easier problems to get myself familiar with the implementation of such a solver using Pytorch. Most of my experience up until recently has been with Tensorflow (I have a Tensorflow Developer certification from Coursera). So far, I’ve enjoyed Pytorch and I’ve found it much more intuitive than Tensorflow."
  },
  {
    "objectID": "Blog/Neural Poisson Solver.html#poisson-equation",
    "href": "Blog/Neural Poisson Solver.html#poisson-equation",
    "title": "Neural Poisson Solver",
    "section": "Poisson Equation",
    "text": "Poisson Equation\nThe first toy problem I present here is the Poisson Equation with Dirichlet boundary condition on the unit disk. I’ve chosen a very simple relationship for \\(\\Delta u\\) and the boundary condition to quickly show that the neural network I train differs only slightly from the analytical solution.\nThe problem I solve is this,\n\\[\\Delta u(x,y)=1\\quad (x,y)\\in B_1(0)\\] \\[u(x,y)=0\\quad (x,y)\\in\\partial B_1(0)\\]\nin \\(\\mathbb{R}^2\\). The analytical solution to this problem is easily seen to be \\(u(x,y)=\\frac{1}{4}(x^2+y^2-1)\\).\n\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\nWe will use a very simple neural network.\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sequential_model = nn.Sequential(\n            nn.Linear(2, 8),\n            nn.Sigmoid(),\n            nn.Linear(8, 1)\n        )\n\n    def forward(self, x):\n        return self.sequential_model(x)\n\nHere, we define a function which samples the necessary data to train the network (and test the network later).\n\ndef SampleFromUnitDisk(points):\n    d = torch.distributions.Uniform(-1,1)\n\n    x = torch.Tensor(points,1)\n    y = torch.Tensor(points,1)\n    j=0\n\n    while j&lt;points:\n        x_temp = d.sample()\n        y_temp = d.sample()\n        if x_temp**2+y_temp**2&lt;1:\n            x[j,0]=x_temp\n            y[j,0]=y_temp\n            j+=1\n\n    xbdry = torch.Tensor(points,1)\n    ybdry = torch.Tensor(points,1)\n    j=0\n\n    #Vary the sign of the y coordinate, for otherwise we'd only have positive y values.\n\n    for j in range(points):\n        x_temp = d.sample()\n        xbdry[j,0]=x_temp\n        if j%2==0:\n            ybdry[j,0]=math.sqrt(1-x_temp**2)\n        else:\n            ybdry[j,0]=-math.sqrt(1-x_temp**2)\n\n    return x, y, xbdry, ybdry\n\nNow, we generate the training data.\n\nx, y, xbdry, ybdry = SampleFromUnitDisk(10000)\n\nLet’s discuss the loss function. Since we’ve descritized the domain, we are going to use a discrete mean-squared error function to compute the loss. In our case, we want to minimize the following,\n\\[L(x_{\\text{int}},y_\\text{int}, x_{\\text{bdry}},y_{\\text{bdry}}):=\\frac{1}{N_{\\text{int}}}\\sum_{j=1}^{N_{\\text{int}}}|\\Delta u(x^{(j)}_{\\text{int}},y^{(j)}_{\\text{int}})-1|^2+\\frac{1}{N_{\\text{bdry}}}\\sum_{j=1}^{N_{\\text{bdry}}}|u(x^{(j)}_{\\text{bdry}},y^{(j)}_{\\text{bdry}})|^2\\]\nThe first term comes from the fact that \\(\\Delta u(x,y)=1\\) on the interior, while \\(u=0\\) identically on the boundary. To implement this, we define the following.\n\ndef loss(x, y, xbdry, ybdry, network):\n    x.requires_grad = True\n    y.requires_grad = True\n    temp_input = torch.cat((x,y),1)\n    z=network(temp_input)\n    zbdry = network(torch.cat((xbdry, ybdry),1))\n\n    dz_dx = torch.autograd.grad(z.sum(), x, create_graph = True)[0]\n    ddz_ddx = torch.autograd.grad(dz_dx.sum(), x, create_graph = True)[0]\n    dz_dy = torch.autograd.grad(z.sum(), y, create_graph = True)[0] \n    ddz_ddy = torch.autograd.grad(dz_dy.sum(), y, create_graph = True)[0]\n\n    return torch.mean((ddz_ddx+ddz_ddy-1)**2)+torch.mean((zbdry-torch.zeros(xbdry.size(0)))**2)\n\nOkay now let’s create our network and train it! We’ll use 2000 epochs.\n\nmodel = NeuralNetwork()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=.9)\n\nepochs = 2000\nloss_values = np.zeros(2000)\nfor i in range(epochs):\n    l = loss(x, y, xbdry, ybdry, model)\n    l.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    loss_values[i]=l\n    if i%100==0:\n        print(\"Loss at epoch {}: {}\".format(i, l.item()))\n\nLoss at epoch 0: 1.0297363996505737\nLoss at epoch 100: 0.7696019411087036\nLoss at epoch 200: 0.10838701575994492\nLoss at epoch 300: 0.040865253657102585\nLoss at epoch 400: 0.02691115066409111\nLoss at epoch 500: 0.02078372612595558\nLoss at epoch 600: 0.017054535448551178\nLoss at epoch 700: 0.0143966656178236\nLoss at epoch 800: 0.012377198785543442\nLoss at epoch 900: 0.010792599990963936\nLoss at epoch 1000: 0.00952119193971157\nLoss at epoch 1100: 0.008482505567371845\nLoss at epoch 1200: 0.007620864547789097\nLoss at epoch 1300: 0.006896625738590956\nLoss at epoch 1400: 0.006280942354351282\nLoss at epoch 1500: 0.005752396769821644\nLoss at epoch 1600: 0.005294801667332649\nLoss at epoch 1700: 0.004895701073110104\nLoss at epoch 1800: 0.00454533938318491\nLoss at epoch 1900: 0.004236005246639252\n\n\nCool, so it seems like we’ve decreased the loss significantly. This is enough for our toy example. Here’s the loss decrease over the course of training:\n\nx_axis = np.linspace(0,2000,2000)[:,None]\nplt.figure(figsize=(5,3))\nplt.plot(x_axis,loss_values,'red', label='loss')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x48cc0d390&gt;\n\n\n\n\n\nNow, let’s simulate a sup-norm test agains the analytic solution, using a test set of data.\n\nx_test, y_test, xbdry_test, ybdry_test = SampleFromUnitDisk(10000)\n\nwith torch.no_grad():\n    z = model(torch.cat((x_test,y_test),1))-(1/4*(x_test**2+y_test**2-1))\n    print(\"Interior sup-norm error: {}\".format(torch.max(abs(z)).item()))\n\nwith torch.no_grad():\n    z = model(torch.cat((xbdry_test,ybdry_test),1))-(1/4*(xbdry_test**2+ybdry_test**2-1))\n    print(\"Boundary sup-norm error: {}\".format(torch.max(abs(z)).item()))\n\nInterior sup-norm error: 0.04881325364112854\nBoundary sup-norm error: 0.04888451099395752\n\n\nOf course, we want to do better, but this is pretty decent for this toy example.\nCool, so we have a baseline implementation for solving PDEs using neural networks. This was of course extremely simple. I started messing with more complicated PDEs (quasilinear, nonlinear) and ran into challenges in both implementation and validation. There are some theoretical questions on how to deal with non-uniqueness of solutions and how to give an “ansatz” when initializaing training. Some of this will probably be the subject of my next post."
  },
  {
    "objectID": "Blog/Deep Learning for a Semilinear Poisson Equation.html",
    "href": "Blog/Deep Learning for a Semilinear Poisson Equation.html",
    "title": "Simple ML Architectures to Solve the Heat Equation",
    "section": "",
    "text": "I decided to try messing with the heat equation in my exploration of using ML to solve PDEs (and take a break from the Poisson equation). The equation itself is simple, as are the architectures I use to solve the equation, but this experiment proved interesting nonetheless. In particular, I found that the accuracy of the solution had more to do with the number and quality of the features I included for input than the architecture itself. I’ll elaborate upon what I mean below."
  },
  {
    "objectID": "Blog/Deep Learning for a Semilinear Poisson Equation.html#heat-equation",
    "href": "Blog/Deep Learning for a Semilinear Poisson Equation.html#heat-equation",
    "title": "Simple ML Architectures to Solve the Heat Equation",
    "section": "Heat Equation",
    "text": "Heat Equation\nI was motivated to solve the heat equation partially because of the nice visualizations that arise from a solution. I include an animation at the end of each solution I describe below. Unfortunately it can only run if the cell is executed; I’m having some environmental issues generating animations that play in-browser that I’ll troubleshoot later. Here’s the rendition of the heat equation I consider:\n\\[\n\\begin{alignat*}{2}\n& u_t(x,y,t)=\\Delta_{(x,y)} u(x,y,t) && (x,y)\\in B_1(0), t&gt;0\\\\\n& u(x,y,t)=0 && (x,y)\\in\\partial B_1(0), t\\geq 0\\\\\n& u(x,y,0)=\\exp\\left(-\\frac{1}{1-(x^2+y^2)}\\right) && (x,y)\\in B_1(0)\n\\end{alignat*}\n\\]\nfor \\(B_1(0)\\subset\\mathbb{R}^2\\). The bump function makes for a particularly nice set of solutions to illustrate. The solution decays quickly, so we will only simulate it for a small period of time.\n\nSimple Neural Network Solution\nFirst, I will use a simple neural network with a single hidden layer. It doesn’t need to be very wide to get pretty good convergence to the solution. Here’s the architecture I use:\n\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nfrom IPython.display import display, clear_output\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sequential_model = nn.Sequential(\n            nn.Linear(3, 8),\n            nn.Sigmoid(),\n            nn.Linear(8, 1)\n        )\n\n    def forward(self, x):\n        return self.sequential_model(x)\n\nBelow is my data sampler function, simililar to the data sampler I used in my post on the Poisson equation. I sample 10000 points in my example and proceed to solve the equation for \\(t\\in\\left[0,0.09\\right]\\).\n\ndef SampleFromUnitBall(points):\n    d = torch.distributions.Uniform(-1,1)\n    d_t = torch.distributions.Uniform(0,0.1)\n\n    x = torch.Tensor(points,1)\n    y = torch.Tensor(points,1)\n    t = torch.Tensor(points,1)\n    j=0\n    k=0\n\n    while j&lt;points:\n        x_temp = d.sample()\n        y_temp = d.sample()\n        t_temp = d_t.sample()\n        if x_temp**2+y_temp**2&lt;1 and t_temp!=0:\n            x[j,0]=x_temp\n            y[j,0]=y_temp\n            t[j,0]=k*1/(10*points)\n            j+=1\n            if j%1000==0 and j!=0:\n                k+=1000\n\n    xbdry = torch.Tensor(points,1)\n    ybdry = torch.Tensor(points,1)\n    tbdry = torch.zeros(x.size(0)).unsqueeze(1)\n    j=0\n    \n    while j&lt;points:\n        x_temp = d.sample()\n        xbdry[j,0]=x_temp\n        if j%2==0:\n            ybdry[j,0]=math.sqrt(1-x_temp**2)\n        else:\n            ybdry[j,0]=-math.sqrt(1-x_temp**2)\n        j+=1\n\n    return x, y, t, xbdry, ybdry, tbdry\n\n\nx, y, t, xbdry, ybdry, tbdry = SampleFromUnitBall(10000)\n\nThe loss function is very similar to what we used in our Poisson solver. Again, I use a discrete mean-squared error function to compute the loss. In this case, I want to minimize the following,\n\\[L(x_{\\text{int}},y_\\text{int}, x_{\\text{bdry}},y_{\\text{bdry}}, t_{&gt;0}):=\\frac{1}{N_{\\text{int}}}\\sum_{j=1}^{N_{\\text{int}}}|\\Delta u(x^{(j)}_{\\text{int}},y^{(j)}_{\\text{int}}, t^{(j)}_{&gt;0})-u(x^{(j)}_{\\text{int}},y^{(j)}_{\\text{int}}, t^{(j)}_{&gt;0})|^2\n\\] \\[\n+\\frac{1}{N_{\\text{bdry}}}\\sum_{j=1}^{N_{\\text{bdry}}}|u(x^{(j)}_{\\text{bdry}},y^{(j)}_{\\text{bdry}},t^{(j)}_{&gt;0})|^2+\\frac{1}{N_{\\text{int}}}\\sum_{j=1}^{N_{\\text{int}}}|u(x^{(j)}_{\\text{int}},y^{(j)}_{\\text{int}},0)-g(x^{(j)}_{\\text{int}},y^{(j)}_{\\text{int}})|^2\n\\]\nwhere \\(g(x,y):=\\exp\\left(-\\frac{1}{1-(x^2+y^2)}\\right)\\).\nTo implement this loss function, I use the following code:\n\ndef g(x,y):\n    return torch.exp(-1/(1-(x**2+y**2)))\n\n\ndef loss(x, y, t, xbdry, ybdry, tbdry, network):\n    x.requires_grad = True\n    y.requires_grad = True\n    t.requires_grad = True\n    temp_input = torch.cat((x,y,t),1)\n    w=network(temp_input)\n    temp_input_init = torch.cat((x,y,tbdry),1)\n    w_init = network(temp_input_init)\n    wbdry = network(torch.cat((xbdry, ybdry, t),1))\n\n    dw_dx = torch.autograd.grad(w.sum(), x, create_graph = True)[0]\n    ddw_ddx = torch.autograd.grad(dw_dx.sum(), x, create_graph = True)[0]\n    dw_dy = torch.autograd.grad(w.sum(), y, create_graph = True)[0] \n    ddw_ddy = torch.autograd.grad(dw_dy.sum(), y, create_graph = True)[0]\n    dw_dt = torch.autograd.grad(w.sum(), t, create_graph = True)[0] \n\n    A = torch.mean((dw_dt - (ddw_ddx+ddw_ddy))**2)\n    B = torch.mean((w_init-g(x,y))**2)\n    C = torch.mean((wbdry)**2)\n\n    return A+B+C\n\nNow I train the network. I use 100000 epochs to get pretty good convergence.\n\nmodel = NeuralNetwork()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=.9)\n\n\nepochs = 100000\nloss_values = np.zeros(100000)\nfor i in range(epochs):\n    l = loss(x, y, t, xbdry, ybdry, tbdry, model)\n    l.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    loss_values[i]=l\n    if i%1000==0:\n        print(\"Loss at epoch {}: {}\".format(i, l.item()))\n\nLoss at epoch 0: 0.2738669812679291\nLoss at epoch 1000: 0.025970257818698883\nLoss at epoch 2000: 0.02568979375064373\nLoss at epoch 3000: 0.025230783969163895\nLoss at epoch 4000: 0.024398991838097572\nLoss at epoch 5000: 0.022912777960300446\nLoss at epoch 6000: 0.02087295800447464\nLoss at epoch 7000: 0.018625637516379356\nLoss at epoch 8000: 0.015907008200883865\nLoss at epoch 9000: 0.013540136627852917\nLoss at epoch 10000: 0.012002145871520042\nLoss at epoch 11000: 0.010946547612547874\nLoss at epoch 12000: 0.01010743249207735\nLoss at epoch 13000: 0.0094174575060606\nLoss at epoch 14000: 0.008831574581563473\nLoss at epoch 15000: 0.008312895894050598\nLoss at epoch 16000: 0.007835050113499165\nLoss at epoch 17000: 0.0073796845972537994\nLoss at epoch 18000: 0.00693441741168499\nLoss at epoch 19000: 0.006492464803159237\nLoss at epoch 20000: 0.006053541321307421\nLoss at epoch 21000: 0.0056252810172736645\nLoss at epoch 22000: 0.0052236150950193405\nLoss at epoch 23000: 0.004869983531534672\nLoss at epoch 24000: 0.004583843518048525\nLoss at epoch 25000: 0.004373359493911266\nLoss at epoch 26000: 0.004231602419167757\nLoss at epoch 27000: 0.004141609184443951\nLoss at epoch 28000: 0.004085080698132515\nLoss at epoch 29000: 0.004047917202115059\nLoss at epoch 30000: 0.004021293483674526\nLoss at epoch 31000: 0.00400038855150342\nLoss at epoch 32000: 0.003982786554843187\nLoss at epoch 33000: 0.0039673191495239735\nLoss at epoch 34000: 0.003953419625759125\nLoss at epoch 35000: 0.00394078902900219\nLoss at epoch 36000: 0.003929249942302704\nLoss at epoch 37000: 0.003918683156371117\nLoss at epoch 38000: 0.003908993676304817\nLoss at epoch 39000: 0.0039000948891043663\nLoss at epoch 40000: 0.003891912754625082\nLoss at epoch 41000: 0.0038843746297061443\nLoss at epoch 42000: 0.0038774253334850073\nLoss at epoch 43000: 0.0038710092194378376\nLoss at epoch 44000: 0.0038650608621537685\nLoss at epoch 45000: 0.003859535790979862\nLoss at epoch 46000: 0.003854390000924468\nLoss at epoch 47000: 0.003849589265882969\nLoss at epoch 48000: 0.00384509633295238\nLoss at epoch 49000: 0.0038408783730119467\nLoss at epoch 50000: 0.0038368944078683853\nLoss at epoch 51000: 0.0038331307005137205\nLoss at epoch 52000: 0.0038295662961900234\nLoss at epoch 53000: 0.003826176282018423\nLoss at epoch 54000: 0.0038229411002248526\nLoss at epoch 55000: 0.003819849109277129\nLoss at epoch 56000: 0.0038168805185705423\nLoss at epoch 57000: 0.0038140309043228626\nLoss at epoch 58000: 0.0038112876936793327\nLoss at epoch 59000: 0.0038086404092609882\nLoss at epoch 60000: 0.0038060781080275774\nLoss at epoch 61000: 0.003803590778261423\nLoss at epoch 62000: 0.003801171202212572\nLoss at epoch 63000: 0.0037988254334777594\nLoss at epoch 64000: 0.0037965471856296062\nLoss at epoch 65000: 0.003794319462031126\nLoss at epoch 66000: 0.0037921415641903877\nLoss at epoch 67000: 0.0037900148890912533\nLoss at epoch 68000: 0.003787929890677333\nLoss at epoch 69000: 0.003785883542150259\nLoss at epoch 70000: 0.003783881664276123\nLoss at epoch 71000: 0.003781920066103339\nLoss at epoch 72000: 0.0037799954880028963\nLoss at epoch 73000: 0.0037781083956360817\nLoss at epoch 74000: 0.003776250407099724\nLoss at epoch 75000: 0.00377441942691803\nLoss at epoch 76000: 0.0037726142909377813\nLoss at epoch 77000: 0.0037708329036831856\nLoss at epoch 78000: 0.0037690771277993917\nLoss at epoch 79000: 0.003767352784052491\nLoss at epoch 80000: 0.003765658475458622\nLoss at epoch 81000: 0.003763986984267831\nLoss at epoch 82000: 0.0037623357493430376\nLoss at epoch 83000: 0.0037607017438858747\nLoss at epoch 84000: 0.003759087296202779\nLoss at epoch 85000: 0.0037574912421405315\nLoss at epoch 86000: 0.003755920333787799\nLoss at epoch 87000: 0.0037543680518865585\nLoss at epoch 88000: 0.0037528336979448795\nLoss at epoch 89000: 0.0037513161078095436\nLoss at epoch 90000: 0.0037498222663998604\nLoss at epoch 91000: 0.0037483531050384045\nLoss at epoch 92000: 0.0037469009403139353\nLoss at epoch 93000: 0.0037454627454280853\nLoss at epoch 94000: 0.003744041081517935\nLoss at epoch 95000: 0.003742641070857644\nLoss at epoch 96000: 0.0037412564270198345\nLoss at epoch 97000: 0.003739887848496437\nLoss at epoch 98000: 0.0037385355681180954\nLoss at epoch 99000: 0.003737200517207384\n\n\nEvidently, one doesn’t need many input features, nor does one need a very wide network to get pretty good convergence. Moreover, I demonstrate the heat flow in the unit circle using the following plots and animation. Here, I’ve plotted a few instances of the solution over test data and three points in time in \\(\\left[0,0.09\\right]\\). Brighter colored scatter points indicate a higher value for \\(u(x,y,t)\\). As expected, the maximum value for \\(u(x,y,t)\\) decreases as time progresses, and the heat diffuses to be nearly constant.\n\nx_test, y_test, z_test, xbdry_test, ybdry_test, zbdry_test = SampleFromUnitBall(10000)\n\ndef plot_points(time_index):\n    with torch.no_grad():\n        temp_input = torch.cat((x_test,y_test,torch.ones(10000).unsqueeze(1)*time_index/100),1)\n        soln=model(temp_input)\n    plt.scatter(x=x_test.detach(),y=y_test.detach(),c=soln.detach(), vmin=0, vmax=0.35)\n    plt.title(\"Solution at time index \" + str(time_index))\n    plt.xlabel(\"x-axis\")\n    plt.ylabel(\"y-axis\")\n    plt.show()\n\nplot_points(0)\nplot_points(3)\nplot_points(9)\n\n\n\n\n\n\n\n\n\n\nRunning the cell below generates an animation of the solutions over time. I can even project the solutions past \\(t=0.09\\) to show that the heat diffusion is accurately captured past our sample times.\n\nx_test, y_test, z_test, xbdry_test, ybdry_test, zbdry_test = SampleFromUnitBall(10000)\nfig, ax = plt.subplots()\nfor k in range(20):\n    with torch.no_grad():\n        temp_input = torch.cat((x_test,y_test,torch.ones(10000).unsqueeze(1)*k/100),1)\n        soln=model(temp_input)\n    ax.cla()\n    ax.scatter(x=x_test.detach(),y=y_test.detach(),c=soln.detach(), vmin=0, vmax=0.35)\n    display(fig)\n    clear_output(wait = True)\n    plt.pause(0.1)\n\n\n\n\n\n\nPolynomial regression solution\nNow, I sample a new set of points and construct a “neural network” which actually allows me to run polynomial regression using my loss function. I show that a degree four polynomial in the input features is suitable for an accurate approximation to a solution of the problem. Here, I take advantage of the fact that the initial condition is radial in its input and admits a nice Taylor expansion in the radial coordinate \\(r^2=x^2+y^2\\). Hence, an accurate approximation of the initial condition may be obtained by using a low-degree polynomial. Interestingly, even though the Taylor series only contains even powers of \\(r\\), including all odd degree terms seemed to improve convergence. Furthermore, the polynomial approximation converges much more rapidly to the actual solution than the nueral network. Essentially, the polynomial approximation contains far more “useful” features than the simple coordinate and time input features I used in the neural network input.\n\nx, y, t, xbdry, ybdry, tbdry = SampleFromUnitBall(10000)\n\n\nclass NeuralNetwork2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sequential_model = nn.Sequential(\n            nn.Linear(32, 1)\n        )\n\n    def forward(self, x):\n        return self.sequential_model(x)\n\nThe loss function is really ugly since I need to compute all polynomial features and their derivatives. I wanted to use scikit-learn’s polynomial feature class, but this destroyed the derivative data in my tensors, so I’d have to create a polynomial feature class from scratch. I might do this in the future, but for now everything is hardcoded.\n\ndef loss2(x, y, t, xbdry, ybdry, tbdry, network):\n    x.requires_grad = True\n    y.requires_grad = True\n    t.requires_grad = True\n\n    #Here come all of the polynomial features....\n\n    x_2 = x**2\n    y_2 = y**2\n    x_3 = x**3\n    y_3 = y**3\n    x_4 = x**4\n    y_4 = y**4\n    xy_2 = x*(y**2)\n    x_2y = (x**2)*y\n    xyt = x*y*t\n    x_2t = (x**2)*t\n    y_2t = (y**2)*t\n    t_3 = t**3\n    t_4 = t**4\n    x_3y = (x**3)*y\n    xy_3 = x*(y**3)\n    x_2y_2 = (x**2)*(y**2)\n    x_3t = (x**3)*t\n    x_2t_2 = (x**2)*(t**2)\n    xt_3 = x*(t**3)\n    y_3t = (y**3)*t\n    y_2t_2 = (y**2)*(t**2)\n    yt_3 = y*(t**3)\n    xyt_2 = x*y*(t**2)\n    xy_2t = x*(y**2)*t\n    x_2yt = (x**2)*y*t\n    xy = x*y\n    t_2 = t**2\n    xt = x*t\n    yt = y*t\n    xbdry_2 = xbdry**2\n    ybdry_2 = ybdry**2\n    xbdry_3 = xbdry**3\n    ybdry_3 = ybdry**3\n    xbdry_4 = xbdry**4\n    ybdry_4 = ybdry**4\n    xbdryybdry_2 = xbdry*(ybdry**2)\n    xbdry_2ybdry = (xbdry**2)*ybdry\n    xbdryybdryt = xbdry*ybdry*t\n    xbdry_2t = (xbdry**2)*t\n    ybdry_2t = (ybdry**2)*t\n    xbdry_ybdry = xbdry*ybdry\n    tbdry_2 = tbdry**2\n    xbdry_t= xbdry*t\n    ybdry_t = ybdry*t\n    x_tbdry = x*tbdry\n    y_tbdry = y*tbdry\n    xytbdry = x*y*tbdry\n    x_2tbdry = (x**2)*tbdry\n    y_2tbdry = (y**2)*tbdry\n    tbdry_3 = tbdry**3\n    tbdry_4 = tbdry**4\n    xbdry_3ybdry = (xbdry**3)*ybdry\n    xbdryybdry_3 = xbdry*(ybdry**3)\n    xbdry_2ybdry_2 = (xbdry**2)*(ybdry**2)\n    xbdry_3t = (xbdry**3)*t\n    xbdry_2t_2 = (xbdry**2)*(t**2)\n    xbdryt_3 = xbdry*(t**3)\n    ybdry_3t = (ybdry**3)*t\n    ybdry_2t_2 = (ybdry**2)*(t**2)\n    ybdryt_3 = ybdry*(t**3)\n    x_3tbdry = (x**3)*tbdry\n    x_2tbdry_2 = (x**2)*(tbdry**2)\n    xtbdry_3 = x*(tbdry**3)\n    y_3tbdry = (y**3)*tbdry\n    y_2tbdry_2 = (y**2)*(tbdry**2)\n    ytbdry_3 = y*(tbdry**3)\n    xbdryybdryt_2 = xbdry*ybdry*(t**2)\n    xbdryybdry_2t = xbdry*(ybdry**2)*t\n    xbdry_2ybdryt = (xbdry**2)*ybdry*t  \n    xytbdry_2 = x*y*(tbdry**2)\n    xy_2tbdry = x*(y**2)*tbdry\n    x_2ytbdry = (x**2)*y*tbdry\n\n    ##End of polynomial features\n\n    temp_input = torch.cat((x,y,t,x_2,y_2,xy,t_2,xt,yt,x_4,y_4,t_4,x_3y,xy_3,x_2y_2,x_3t,x_2t_2,xt_3,y_3t,y_2t_2,yt_3,xyt_2,xy_2t,x_2yt,x_3,y_3,xy_2,x_2y,xyt,x_2t,y_2t,t_3),1)\n    w=network(temp_input)\n    temp_input_init = torch.cat((x,y,tbdry,x_2,y_2,xy,tbdry_2,x_tbdry,y_tbdry,x_4,y_4,tbdry_4,x_3y,xy_3,x_2y_2,x_3tbdry,x_2tbdry_2,xtbdry_3,y_3tbdry,y_2tbdry_2,ytbdry_3,xytbdry_2,xy_2tbdry,x_2ytbdry,x_3,y_3,xy_2,x_2y,xytbdry,x_2tbdry,y_2tbdry,tbdry_3),1)\n    w_init = network(temp_input_init)\n    wbdry = network(torch.cat((xbdry, ybdry, t,xbdry_2,ybdry_2,xbdry_ybdry,t_2,xbdry_t,ybdry_t,xbdry_4,ybdry_4,t_4,xbdry_3ybdry,xbdryybdry_3,xbdry_2ybdry_2,xbdry_3t,xbdry_2t_2,xbdryt_3,ybdry_3t,ybdry_2t_2,ybdryt_3,xbdryybdryt_2,xbdryybdry_2t,xbdry_2ybdryt,xbdry_3,ybdry_3,xbdryybdry_2,xbdry_2ybdry,xbdryybdryt,xbdry_2t,ybdry_2t,t_3),1))\n\n    dw_dx = torch.autograd.grad(w.sum(), x, create_graph = True)[0]\n    ddw_ddx = torch.autograd.grad(dw_dx.sum(), x, create_graph = True)[0]\n    dw_dy = torch.autograd.grad(w.sum(), y, create_graph = True)[0] \n    ddw_ddy = torch.autograd.grad(dw_dy.sum(), y, create_graph = True)[0]\n    dw_dt = torch.autograd.grad(w.sum(), t, create_graph = True)[0] \n\n    A = torch.mean((dw_dt - (ddw_ddx+ddw_ddy))**2)\n    B = torch.mean((w_init-g(x,y))**2)\n    C = torch.mean((wbdry)**2)\n    #print(\"Loss for each item: {}, {}, {}\".format(A,B,C))\n    return A+B+C\n\nBelow, I define a helper function for computing values the network generates from input data. It computes all of the needed polynomial features before supplying them into the network.\n\ndef network_value(x, y, t, network):\n    x_2 = x**2\n    y_2 = y**2\n    x_3 = x**3\n    y_3 = y**3\n    x_4 = x**4\n    y_4 = y**4\n    xy_2 = x*(y**2)\n    x_2y = (x**2)*y\n    xyt = x*y*t\n    x_2t = (x**2)*t\n    y_2t = (y**2)*t\n    t_3 = t**3\n    t_4 = t**4\n    xy = x*y\n    t_2 = t**2\n    xt = x*t\n    yt = y*t\n    x_3y = (x**3)*y\n    xy_3 = x*(y**3)\n    x_2y_2 = (x**2)*(y**2)\n    x_3t = (x**3)*t\n    x_2t_2 = (x**2)*(t**2)\n    xt_3 = x*(t**3)\n    y_3t = (y**3)*t\n    y_2t_2 = (y**2)*(t**2)\n    yt_3 = y*(t**3)\n    xyt_2 = x*y*(t**2)\n    xy_2t = x*(y**2)*t\n    x_2yt = (x**2)*y*t\n    temp_input = torch.cat((x,y,t,x_2,y_2,xy,t_2,xt,yt,x_4,y_4,t_4,x_3y,xy_3,x_2y_2,x_3t,x_2t_2,xt_3,y_3t,y_2t_2,yt_3,xyt_2,xy_2t,x_2yt,x_3,y_3,xy_2,x_2y,xyt,x_2t,y_2t,t_3),1)\n    w=network(temp_input)\n    return w\n\nNow, I build and train the network.\n\nmodel2 = NeuralNetwork2()\noptimizer2 = torch.optim.SGD(model2.parameters(), lr=0.01, momentum=.9)\n\n\nepochs = 100000\nloss_values = np.zeros(100000)\nfor i in range(epochs):\n    l = loss2(x, y, t, xbdry, ybdry, tbdry, model2)\n    l.backward()\n    optimizer2.step()\n    optimizer2.zero_grad()\n    loss_values[i]=l\n    if i%1000==0:\n        print(\"Loss at epoch {}: {}\".format(i, l.item()))\n\nLoss at epoch 0: 0.89188551902771\nLoss at epoch 1000: 0.003854417707771063\nLoss at epoch 2000: 0.0035820240154862404\nLoss at epoch 3000: 0.003396423067897558\nLoss at epoch 4000: 0.0032313335686922073\nLoss at epoch 5000: 0.003081361996009946\nLoss at epoch 6000: 0.0029437472112476826\nLoss at epoch 7000: 0.0028166514821350574\nLoss at epoch 8000: 0.0026987330056726933\nLoss at epoch 9000: 0.002588952425867319\nLoss at epoch 10000: 0.002486460143700242\nLoss at epoch 11000: 0.0023905490525066853\nLoss at epoch 12000: 0.0023006070405244827\nLoss at epoch 13000: 0.0022161088418215513\nLoss at epoch 14000: 0.002136589726433158\nLoss at epoch 15000: 0.0020616455003619194\nLoss at epoch 16000: 0.0019909129478037357\nLoss at epoch 17000: 0.001924069132655859\nLoss at epoch 18000: 0.0018608317477628589\nLoss at epoch 19000: 0.0018009372288361192\nLoss at epoch 20000: 0.0017441592644900084\nLoss at epoch 21000: 0.001690280856564641\nLoss at epoch 22000: 0.0016391162062063813\nLoss at epoch 23000: 0.0015904909232631326\nLoss at epoch 24000: 0.0015442450530827045\nLoss at epoch 25000: 0.0015002338914200664\nLoss at epoch 26000: 0.0014583258889615536\nLoss at epoch 27000: 0.0014183972962200642\nLoss at epoch 28000: 0.0013803349575027823\nLoss at epoch 29000: 0.0013440358452498913\nLoss at epoch 30000: 0.0013093998422846198\nLoss at epoch 31000: 0.0012763398699462414\nLoss at epoch 32000: 0.0012447721092030406\nLoss at epoch 33000: 0.0012146164663136005\nLoss at epoch 34000: 0.0011857992503792048\nLoss at epoch 35000: 0.0011582525912672281\nLoss at epoch 36000: 0.0011319173499941826\nLoss at epoch 37000: 0.0011067262385040522\nLoss at epoch 38000: 0.001082623261027038\nLoss at epoch 39000: 0.001059556845575571\nLoss at epoch 40000: 0.0010374787962064147\nLoss at epoch 41000: 0.0010163375409319997\nLoss at epoch 42000: 0.000996090704575181\nLoss at epoch 43000: 0.0009766991715878248\nLoss at epoch 44000: 0.0009581181220710278\nLoss at epoch 45000: 0.0009403128060512245\nLoss at epoch 46000: 0.0009232479496859014\nLoss at epoch 47000: 0.0009068892104551196\nLoss at epoch 48000: 0.0008912016055546701\nLoss at epoch 49000: 0.0008761570206843317\nLoss at epoch 50000: 0.0008617275161668658\nLoss at epoch 51000: 0.0008478870149701834\nLoss at epoch 52000: 0.0008346029790118337\nLoss at epoch 53000: 0.0008218575385399163\nLoss at epoch 54000: 0.0008096238598227501\nLoss at epoch 55000: 0.0007978778448887169\nLoss at epoch 56000: 0.0007866068044677377\nLoss at epoch 57000: 0.0007757834391668439\nLoss at epoch 58000: 0.0007653866778127849\nLoss at epoch 59000: 0.0007554012699984014\nLoss at epoch 60000: 0.0007458093459717929\nLoss at epoch 61000: 0.0007365943747572601\nLoss at epoch 62000: 0.0007277417462319136\nLoss at epoch 63000: 0.0007192345801740885\nLoss at epoch 64000: 0.0007110605365596712\nLoss at epoch 65000: 0.0007032029679976404\nLoss at epoch 66000: 0.0006956501747481525\nLoss at epoch 67000: 0.0006883873138576746\nLoss at epoch 68000: 0.0006814048392698169\nLoss at epoch 69000: 0.0006746951257809997\nLoss at epoch 70000: 0.0006682390812784433\nLoss at epoch 71000: 0.0006620289059355855\nLoss at epoch 72000: 0.0006560610490851104\nLoss at epoch 73000: 0.0006503157783299685\nLoss at epoch 74000: 0.0006447912310250103\nLoss at epoch 75000: 0.0006394768133759499\nLoss at epoch 76000: 0.0006343616987578571\nLoss at epoch 77000: 0.0006294394261203706\nLoss at epoch 78000: 0.0006247067940421402\nLoss at epoch 79000: 0.0006201465730555356\nLoss at epoch 80000: 0.0006157630705274642\nLoss at epoch 81000: 0.0006115416181273758\nLoss at epoch 82000: 0.0006074761622585356\nLoss at epoch 83000: 0.0006035627448000014\nLoss at epoch 84000: 0.0005997983971610665\nLoss at epoch 85000: 0.0005961735732853413\nLoss at epoch 86000: 0.000592682627029717\nLoss at epoch 87000: 0.000589318631682545\nLoss at epoch 88000: 0.0005860833334736526\nLoss at epoch 89000: 0.0005829664296470582\nLoss at epoch 90000: 0.0005799678037874401\nLoss at epoch 91000: 0.0005770765710622072\nLoss at epoch 92000: 0.0005742927896790206\nLoss at epoch 93000: 0.0005716124433092773\nLoss at epoch 94000: 0.0005690286634489894\nLoss at epoch 95000: 0.0005665384232997894\nLoss at epoch 96000: 0.0005641438765451312\nLoss at epoch 97000: 0.0005618334980681539\nLoss at epoch 98000: 0.0005596067057922482\nLoss at epoch 99000: 0.0005574636743403971\n\n\nEvidently, the convergence to a solution is much more rapid (one needs far fewer epochs to obtain the same loss as in the more traditional neural network). Below are plots of the solution, as well as an animation. These look virtually identical to the results using the more traditional neural network.\n\nx_test, y_test, z_test, xbdry_test, ybdry_test, zbdry_test = SampleFromUnitBall(10000)\n\ndef plot_points2(time_index):\n    with torch.no_grad():\n        soln = network_value(x_test,y_test,torch.ones(10000).unsqueeze(1)*time_index/100,model2)\n    plt.scatter(x=x_test.detach(),y=y_test.detach(),c=soln.detach(), vmin=0, vmax=0.35)\n    plt.title(\"Solution at time index \" + str(time_index))\n    plt.xlabel(\"x-axis\")\n    plt.ylabel(\"y-axis\")\n    plt.show()\n\nplot_points2(0)\nplot_points2(3)\nplot_points2(9)\n\n\n\n\n\n\n\n\n\n\nInterestingly, projecting the solution into the future past \\(t=0.09\\) seems to yield a less accurate solution than the one I got from the neural network. But still, this isn’t bad and leads to some interesting theoretical questions as to the degree of polynomial required to get “future in time” predictions which are as accurate as the network.\n\nx_test, y_test, z_test, xbdry_test, ybdry_test, zbdry_test = SampleFromUnitBall(10000)\nfig, ax = plt.subplots()\nfor k in range(20):\n    with torch.no_grad():\n        soln = network_value(x_test,y_test,torch.ones(10000).unsqueeze(1)*k/100,model2)\n    ax.cla()\n    ax.scatter(x=x_test.detach(),y=y_test.detach(),c=soln.detach(), vmin=0, vmax=0.35)\n    display(fig)\n    clear_output(wait = True)\n    plt.pause(0.1)\n\n\n\n\nNow that I have all of this set up, it would be interesting to run the code against more complicated initial conditions. I leave this exercise to the reader. Next time, I think I’ll get back to something more like the Poisson equation, or perhaps work on solving PDEs which arise from variational problems. Overall, solving PDEs using neural networks or polynomial regression is proving productive and interesting!"
  },
  {
    "objectID": "Blog/Heat Equation.html",
    "href": "Blog/Heat Equation.html",
    "title": "Simple ML Architectures to Solve the Heat Equation",
    "section": "",
    "text": "I decided to try messing with the heat equation in my exploration of using ML to solve PDEs (and take a break from the Poisson equation). The equation itself is simple, as are the architectures I use to solve the equation, but this experiment proved interesting nonetheless. In particular, I found that the accuracy of the solution had more to do with the number and quality of the features I included for input than the architecture itself. I’ll elaborate upon what I mean below."
  },
  {
    "objectID": "Blog/Heat Equation.html#heat-equation",
    "href": "Blog/Heat Equation.html#heat-equation",
    "title": "Simple ML Architectures to Solve the Heat Equation",
    "section": "Heat Equation",
    "text": "Heat Equation\nI was motivated to solve the heat equation partially because of the nice visualizations that arise from a solution. I include an animation at the end of each solution I describe below. Unfortunately it can only run if the cell is executed; I’m having some environmental issues generating animations that play in-browser that I’ll troubleshoot later. Here’s the rendition of the heat equation I consider:\n\\[\n\\begin{alignat*}{2}\n& u_t(x,y,t)=\\Delta_{(x,y)} u(x,y,t) && (x,y)\\in B_1(0), t&gt;0\\\\\n& u(x,y,t)=0 && (x,y)\\in\\partial B_1(0), t\\geq 0\\\\\n& u(x,y,0)=\\exp\\left(-\\frac{1}{1-(x^2+y^2)}\\right) && (x,y)\\in B_1(0)\n\\end{alignat*}\n\\]\nfor \\(B_1(0)\\subset\\mathbb{R}^2\\). The bump function makes for a particularly nice set of solutions to illustrate. The solution decays quickly, so we will only simulate it for a small period of time.\n\nSimple Neural Network Solution\nFirst, I will use a simple neural network with a single hidden layer. It doesn’t need to be very wide to get pretty good convergence to the solution. Here’s the architecture I use:\n\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nfrom IPython.display import display, clear_output\n\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sequential_model = nn.Sequential(\n            nn.Linear(3, 8),\n            nn.Sigmoid(),\n            nn.Linear(8, 1)\n        )\n\n    def forward(self, x):\n        return self.sequential_model(x)\n\nBelow is my data sampler function, simililar to the data sampler I used in my post on the Poisson equation. I sample 10000 points in my example and proceed to solve the equation for \\(t\\in\\left[0,0.09\\right]\\).\n\ndef SampleFromUnitBall(points):\n    d = torch.distributions.Uniform(-1,1)\n    d_t = torch.distributions.Uniform(0,0.1)\n\n    x = torch.Tensor(points,1)\n    y = torch.Tensor(points,1)\n    t = torch.Tensor(points,1)\n    j=0\n    k=0\n\n    while j&lt;points:\n        x_temp = d.sample()\n        y_temp = d.sample()\n        t_temp = d_t.sample()\n        if x_temp**2+y_temp**2&lt;1 and t_temp!=0:\n            x[j,0]=x_temp\n            y[j,0]=y_temp\n            t[j,0]=k*1/(10*points)\n            j+=1\n            if j%1000==0 and j!=0:\n                k+=1000\n\n    xbdry = torch.Tensor(points,1)\n    ybdry = torch.Tensor(points,1)\n    tbdry = torch.zeros(x.size(0)).unsqueeze(1)\n    j=0\n    \n    while j&lt;points:\n        x_temp = d.sample()\n        xbdry[j,0]=x_temp\n        if j%2==0:\n            ybdry[j,0]=math.sqrt(1-x_temp**2)\n        else:\n            ybdry[j,0]=-math.sqrt(1-x_temp**2)\n        j+=1\n\n    return x, y, t, xbdry, ybdry, tbdry\n\n\nx, y, t, xbdry, ybdry, tbdry = SampleFromUnitBall(10000)\n\nThe loss function is very similar to what we used in our Poisson solver. Again, I use a discrete mean-squared error function to compute the loss. In this case, I want to minimize the following,\n\\[L(x_{\\text{int}},y_\\text{int}, x_{\\text{bdry}},y_{\\text{bdry}}, t_{&gt;0}):=\\frac{1}{N_{\\text{int}}}\\sum_{j=1}^{N_{\\text{int}}}|\\Delta u(x^{(j)}_{\\text{int}},y^{(j)}_{\\text{int}}, t^{(j)}_{&gt;0})-u(x^{(j)}_{\\text{int}},y^{(j)}_{\\text{int}}, t^{(j)}_{&gt;0})|^2\n\\] \\[\n+\\frac{1}{N_{\\text{bdry}}}\\sum_{j=1}^{N_{\\text{bdry}}}|u(x^{(j)}_{\\text{bdry}},y^{(j)}_{\\text{bdry}},t^{(j)}_{&gt;0})|^2+\\frac{1}{N_{\\text{int}}}\\sum_{j=1}^{N_{\\text{int}}}|u(x^{(j)}_{\\text{int}},y^{(j)}_{\\text{int}},0)-g(x^{(j)}_{\\text{int}},y^{(j)}_{\\text{int}})|^2\n\\]\nwhere \\(g(x,y):=\\exp\\left(-\\frac{1}{1-(x^2+y^2)}\\right)\\).\nTo implement this loss function, I use the following code:\n\ndef g(x,y):\n    return torch.exp(-1/(1-(x**2+y**2)))\n\n\ndef loss(x, y, t, xbdry, ybdry, tbdry, network):\n    x.requires_grad = True\n    y.requires_grad = True\n    t.requires_grad = True\n    temp_input = torch.cat((x,y,t),1)\n    w=network(temp_input)\n    temp_input_init = torch.cat((x,y,tbdry),1)\n    w_init = network(temp_input_init)\n    wbdry = network(torch.cat((xbdry, ybdry, t),1))\n\n    dw_dx = torch.autograd.grad(w.sum(), x, create_graph = True)[0]\n    ddw_ddx = torch.autograd.grad(dw_dx.sum(), x, create_graph = True)[0]\n    dw_dy = torch.autograd.grad(w.sum(), y, create_graph = True)[0] \n    ddw_ddy = torch.autograd.grad(dw_dy.sum(), y, create_graph = True)[0]\n    dw_dt = torch.autograd.grad(w.sum(), t, create_graph = True)[0] \n\n    A = torch.mean((dw_dt - (ddw_ddx+ddw_ddy))**2)\n    B = torch.mean((w_init-g(x,y))**2)\n    C = torch.mean((wbdry)**2)\n\n    return A+B+C\n\nNow I train the network. I use 100000 epochs to get pretty good convergence.\n\nmodel = NeuralNetwork()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=.9)\n\n\nepochs = 100000\nloss_values = np.zeros(100000)\nfor i in range(epochs):\n    l = loss(x, y, t, xbdry, ybdry, tbdry, model)\n    l.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    loss_values[i]=l\n    if i%1000==0:\n        print(\"Loss at epoch {}: {}\".format(i, l.item()))\n\nLoss at epoch 0: 0.2738669812679291\nLoss at epoch 1000: 0.025970257818698883\nLoss at epoch 2000: 0.02568979375064373\nLoss at epoch 3000: 0.025230783969163895\nLoss at epoch 4000: 0.024398991838097572\nLoss at epoch 5000: 0.022912777960300446\nLoss at epoch 6000: 0.02087295800447464\nLoss at epoch 7000: 0.018625637516379356\nLoss at epoch 8000: 0.015907008200883865\nLoss at epoch 9000: 0.013540136627852917\nLoss at epoch 10000: 0.012002145871520042\nLoss at epoch 11000: 0.010946547612547874\nLoss at epoch 12000: 0.01010743249207735\nLoss at epoch 13000: 0.0094174575060606\nLoss at epoch 14000: 0.008831574581563473\nLoss at epoch 15000: 0.008312895894050598\nLoss at epoch 16000: 0.007835050113499165\nLoss at epoch 17000: 0.0073796845972537994\nLoss at epoch 18000: 0.00693441741168499\nLoss at epoch 19000: 0.006492464803159237\nLoss at epoch 20000: 0.006053541321307421\nLoss at epoch 21000: 0.0056252810172736645\nLoss at epoch 22000: 0.0052236150950193405\nLoss at epoch 23000: 0.004869983531534672\nLoss at epoch 24000: 0.004583843518048525\nLoss at epoch 25000: 0.004373359493911266\nLoss at epoch 26000: 0.004231602419167757\nLoss at epoch 27000: 0.004141609184443951\nLoss at epoch 28000: 0.004085080698132515\nLoss at epoch 29000: 0.004047917202115059\nLoss at epoch 30000: 0.004021293483674526\nLoss at epoch 31000: 0.00400038855150342\nLoss at epoch 32000: 0.003982786554843187\nLoss at epoch 33000: 0.0039673191495239735\nLoss at epoch 34000: 0.003953419625759125\nLoss at epoch 35000: 0.00394078902900219\nLoss at epoch 36000: 0.003929249942302704\nLoss at epoch 37000: 0.003918683156371117\nLoss at epoch 38000: 0.003908993676304817\nLoss at epoch 39000: 0.0039000948891043663\nLoss at epoch 40000: 0.003891912754625082\nLoss at epoch 41000: 0.0038843746297061443\nLoss at epoch 42000: 0.0038774253334850073\nLoss at epoch 43000: 0.0038710092194378376\nLoss at epoch 44000: 0.0038650608621537685\nLoss at epoch 45000: 0.003859535790979862\nLoss at epoch 46000: 0.003854390000924468\nLoss at epoch 47000: 0.003849589265882969\nLoss at epoch 48000: 0.00384509633295238\nLoss at epoch 49000: 0.0038408783730119467\nLoss at epoch 50000: 0.0038368944078683853\nLoss at epoch 51000: 0.0038331307005137205\nLoss at epoch 52000: 0.0038295662961900234\nLoss at epoch 53000: 0.003826176282018423\nLoss at epoch 54000: 0.0038229411002248526\nLoss at epoch 55000: 0.003819849109277129\nLoss at epoch 56000: 0.0038168805185705423\nLoss at epoch 57000: 0.0038140309043228626\nLoss at epoch 58000: 0.0038112876936793327\nLoss at epoch 59000: 0.0038086404092609882\nLoss at epoch 60000: 0.0038060781080275774\nLoss at epoch 61000: 0.003803590778261423\nLoss at epoch 62000: 0.003801171202212572\nLoss at epoch 63000: 0.0037988254334777594\nLoss at epoch 64000: 0.0037965471856296062\nLoss at epoch 65000: 0.003794319462031126\nLoss at epoch 66000: 0.0037921415641903877\nLoss at epoch 67000: 0.0037900148890912533\nLoss at epoch 68000: 0.003787929890677333\nLoss at epoch 69000: 0.003785883542150259\nLoss at epoch 70000: 0.003783881664276123\nLoss at epoch 71000: 0.003781920066103339\nLoss at epoch 72000: 0.0037799954880028963\nLoss at epoch 73000: 0.0037781083956360817\nLoss at epoch 74000: 0.003776250407099724\nLoss at epoch 75000: 0.00377441942691803\nLoss at epoch 76000: 0.0037726142909377813\nLoss at epoch 77000: 0.0037708329036831856\nLoss at epoch 78000: 0.0037690771277993917\nLoss at epoch 79000: 0.003767352784052491\nLoss at epoch 80000: 0.003765658475458622\nLoss at epoch 81000: 0.003763986984267831\nLoss at epoch 82000: 0.0037623357493430376\nLoss at epoch 83000: 0.0037607017438858747\nLoss at epoch 84000: 0.003759087296202779\nLoss at epoch 85000: 0.0037574912421405315\nLoss at epoch 86000: 0.003755920333787799\nLoss at epoch 87000: 0.0037543680518865585\nLoss at epoch 88000: 0.0037528336979448795\nLoss at epoch 89000: 0.0037513161078095436\nLoss at epoch 90000: 0.0037498222663998604\nLoss at epoch 91000: 0.0037483531050384045\nLoss at epoch 92000: 0.0037469009403139353\nLoss at epoch 93000: 0.0037454627454280853\nLoss at epoch 94000: 0.003744041081517935\nLoss at epoch 95000: 0.003742641070857644\nLoss at epoch 96000: 0.0037412564270198345\nLoss at epoch 97000: 0.003739887848496437\nLoss at epoch 98000: 0.0037385355681180954\nLoss at epoch 99000: 0.003737200517207384\n\n\nEvidently, one doesn’t need many input features, nor does one need a very wide network to get pretty good convergence. Moreover, I demonstrate the heat flow in the unit circle using the following plots and animation. Here, I’ve plotted a few instances of the solution over test data and three points in time in \\(\\left[0,0.09\\right]\\). Brighter colored scatter points indicate a higher value for \\(u(x,y,t)\\). As expected, the maximum value for \\(u(x,y,t)\\) decreases as time progresses, and the heat diffuses to be nearly constant.\n\nx_test, y_test, z_test, xbdry_test, ybdry_test, zbdry_test = SampleFromUnitBall(10000)\n\ndef plot_points(time_index):\n    with torch.no_grad():\n        temp_input = torch.cat((x_test,y_test,torch.ones(10000).unsqueeze(1)*time_index/100),1)\n        soln=model(temp_input)\n    plt.scatter(x=x_test.detach(),y=y_test.detach(),c=soln.detach(), vmin=0, vmax=0.35)\n    plt.title(\"Solution at time index \" + str(time_index))\n    plt.xlabel(\"x-axis\")\n    plt.ylabel(\"y-axis\")\n    plt.show()\n\nplot_points(0)\nplot_points(3)\nplot_points(9)\n\n\n\n\n\n\n\n\n\n\nRunning the cell below generates an animation of the solutions over time. I can even project the solutions past \\(t=0.09\\) to show that the heat diffusion is accurately captured past our sample times.\n\nx_test, y_test, z_test, xbdry_test, ybdry_test, zbdry_test = SampleFromUnitBall(10000)\nfig, ax = plt.subplots()\nfor k in range(20):\n    with torch.no_grad():\n        temp_input = torch.cat((x_test,y_test,torch.ones(10000).unsqueeze(1)*k/100),1)\n        soln=model(temp_input)\n    ax.cla()\n    ax.scatter(x=x_test.detach(),y=y_test.detach(),c=soln.detach(), vmin=0, vmax=0.35)\n    display(fig)\n    clear_output(wait = True)\n    plt.pause(0.1)\n\n\n\n\n\n\nPolynomial regression solution\nNow, I sample a new set of points and construct a “neural network” which actually allows me to run polynomial regression using my loss function. I show that a degree four polynomial in the input features is suitable for an accurate approximation to a solution of the problem. Here, I take advantage of the fact that the initial condition is radial in its input and admits a nice Taylor expansion in the radial coordinate \\(r^2=x^2+y^2\\). Hence, an accurate approximation of the initial condition may be obtained by using a low-degree polynomial. Interestingly, even though the Taylor series only contains even powers of \\(r\\), including all odd degree terms seemed to improve convergence. Furthermore, the polynomial approximation converges much more rapidly to the actual solution than the nueral network. Essentially, the polynomial approximation contains far more “useful” features than the simple coordinate and time input features I used in the neural network input.\n\nx, y, t, xbdry, ybdry, tbdry = SampleFromUnitBall(10000)\n\n\nclass NeuralNetwork2(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sequential_model = nn.Sequential(\n            nn.Linear(32, 1)\n        )\n\n    def forward(self, x):\n        return self.sequential_model(x)\n\nThe loss function is really ugly since I need to compute all polynomial features and their derivatives. I wanted to use scikit-learn’s polynomial feature class, but this destroyed the derivative data in my tensors, so I’d have to create a polynomial feature class from scratch. I might do this in the future, but for now everything is hardcoded.\n\ndef loss2(x, y, t, xbdry, ybdry, tbdry, network):\n    x.requires_grad = True\n    y.requires_grad = True\n    t.requires_grad = True\n\n    #Here come all of the polynomial features....\n\n    x_2 = x**2\n    y_2 = y**2\n    x_3 = x**3\n    y_3 = y**3\n    x_4 = x**4\n    y_4 = y**4\n    xy_2 = x*(y**2)\n    x_2y = (x**2)*y\n    xyt = x*y*t\n    x_2t = (x**2)*t\n    y_2t = (y**2)*t\n    t_3 = t**3\n    t_4 = t**4\n    x_3y = (x**3)*y\n    xy_3 = x*(y**3)\n    x_2y_2 = (x**2)*(y**2)\n    x_3t = (x**3)*t\n    x_2t_2 = (x**2)*(t**2)\n    xt_3 = x*(t**3)\n    y_3t = (y**3)*t\n    y_2t_2 = (y**2)*(t**2)\n    yt_3 = y*(t**3)\n    xyt_2 = x*y*(t**2)\n    xy_2t = x*(y**2)*t\n    x_2yt = (x**2)*y*t\n    xy = x*y\n    t_2 = t**2\n    xt = x*t\n    yt = y*t\n    xbdry_2 = xbdry**2\n    ybdry_2 = ybdry**2\n    xbdry_3 = xbdry**3\n    ybdry_3 = ybdry**3\n    xbdry_4 = xbdry**4\n    ybdry_4 = ybdry**4\n    xbdryybdry_2 = xbdry*(ybdry**2)\n    xbdry_2ybdry = (xbdry**2)*ybdry\n    xbdryybdryt = xbdry*ybdry*t\n    xbdry_2t = (xbdry**2)*t\n    ybdry_2t = (ybdry**2)*t\n    xbdry_ybdry = xbdry*ybdry\n    tbdry_2 = tbdry**2\n    xbdry_t= xbdry*t\n    ybdry_t = ybdry*t\n    x_tbdry = x*tbdry\n    y_tbdry = y*tbdry\n    xytbdry = x*y*tbdry\n    x_2tbdry = (x**2)*tbdry\n    y_2tbdry = (y**2)*tbdry\n    tbdry_3 = tbdry**3\n    tbdry_4 = tbdry**4\n    xbdry_3ybdry = (xbdry**3)*ybdry\n    xbdryybdry_3 = xbdry*(ybdry**3)\n    xbdry_2ybdry_2 = (xbdry**2)*(ybdry**2)\n    xbdry_3t = (xbdry**3)*t\n    xbdry_2t_2 = (xbdry**2)*(t**2)\n    xbdryt_3 = xbdry*(t**3)\n    ybdry_3t = (ybdry**3)*t\n    ybdry_2t_2 = (ybdry**2)*(t**2)\n    ybdryt_3 = ybdry*(t**3)\n    x_3tbdry = (x**3)*tbdry\n    x_2tbdry_2 = (x**2)*(tbdry**2)\n    xtbdry_3 = x*(tbdry**3)\n    y_3tbdry = (y**3)*tbdry\n    y_2tbdry_2 = (y**2)*(tbdry**2)\n    ytbdry_3 = y*(tbdry**3)\n    xbdryybdryt_2 = xbdry*ybdry*(t**2)\n    xbdryybdry_2t = xbdry*(ybdry**2)*t\n    xbdry_2ybdryt = (xbdry**2)*ybdry*t  \n    xytbdry_2 = x*y*(tbdry**2)\n    xy_2tbdry = x*(y**2)*tbdry\n    x_2ytbdry = (x**2)*y*tbdry\n\n    ##End of polynomial features\n\n    temp_input = torch.cat((x,y,t,x_2,y_2,xy,t_2,xt,yt,x_4,y_4,t_4,x_3y,xy_3,x_2y_2,x_3t,x_2t_2,xt_3,y_3t,y_2t_2,yt_3,xyt_2,xy_2t,x_2yt,x_3,y_3,xy_2,x_2y,xyt,x_2t,y_2t,t_3),1)\n    w=network(temp_input)\n    temp_input_init = torch.cat((x,y,tbdry,x_2,y_2,xy,tbdry_2,x_tbdry,y_tbdry,x_4,y_4,tbdry_4,x_3y,xy_3,x_2y_2,x_3tbdry,x_2tbdry_2,xtbdry_3,y_3tbdry,y_2tbdry_2,ytbdry_3,xytbdry_2,xy_2tbdry,x_2ytbdry,x_3,y_3,xy_2,x_2y,xytbdry,x_2tbdry,y_2tbdry,tbdry_3),1)\n    w_init = network(temp_input_init)\n    wbdry = network(torch.cat((xbdry, ybdry, t,xbdry_2,ybdry_2,xbdry_ybdry,t_2,xbdry_t,ybdry_t,xbdry_4,ybdry_4,t_4,xbdry_3ybdry,xbdryybdry_3,xbdry_2ybdry_2,xbdry_3t,xbdry_2t_2,xbdryt_3,ybdry_3t,ybdry_2t_2,ybdryt_3,xbdryybdryt_2,xbdryybdry_2t,xbdry_2ybdryt,xbdry_3,ybdry_3,xbdryybdry_2,xbdry_2ybdry,xbdryybdryt,xbdry_2t,ybdry_2t,t_3),1))\n\n    dw_dx = torch.autograd.grad(w.sum(), x, create_graph = True)[0]\n    ddw_ddx = torch.autograd.grad(dw_dx.sum(), x, create_graph = True)[0]\n    dw_dy = torch.autograd.grad(w.sum(), y, create_graph = True)[0] \n    ddw_ddy = torch.autograd.grad(dw_dy.sum(), y, create_graph = True)[0]\n    dw_dt = torch.autograd.grad(w.sum(), t, create_graph = True)[0] \n\n    A = torch.mean((dw_dt - (ddw_ddx+ddw_ddy))**2)\n    B = torch.mean((w_init-g(x,y))**2)\n    C = torch.mean((wbdry)**2)\n    #print(\"Loss for each item: {}, {}, {}\".format(A,B,C))\n    return A+B+C\n\nBelow, I define a helper function for computing values the network generates from input data. It computes all of the needed polynomial features before supplying them into the network.\n\ndef network_value(x, y, t, network):\n    x_2 = x**2\n    y_2 = y**2\n    x_3 = x**3\n    y_3 = y**3\n    x_4 = x**4\n    y_4 = y**4\n    xy_2 = x*(y**2)\n    x_2y = (x**2)*y\n    xyt = x*y*t\n    x_2t = (x**2)*t\n    y_2t = (y**2)*t\n    t_3 = t**3\n    t_4 = t**4\n    xy = x*y\n    t_2 = t**2\n    xt = x*t\n    yt = y*t\n    x_3y = (x**3)*y\n    xy_3 = x*(y**3)\n    x_2y_2 = (x**2)*(y**2)\n    x_3t = (x**3)*t\n    x_2t_2 = (x**2)*(t**2)\n    xt_3 = x*(t**3)\n    y_3t = (y**3)*t\n    y_2t_2 = (y**2)*(t**2)\n    yt_3 = y*(t**3)\n    xyt_2 = x*y*(t**2)\n    xy_2t = x*(y**2)*t\n    x_2yt = (x**2)*y*t\n    temp_input = torch.cat((x,y,t,x_2,y_2,xy,t_2,xt,yt,x_4,y_4,t_4,x_3y,xy_3,x_2y_2,x_3t,x_2t_2,xt_3,y_3t,y_2t_2,yt_3,xyt_2,xy_2t,x_2yt,x_3,y_3,xy_2,x_2y,xyt,x_2t,y_2t,t_3),1)\n    w=network(temp_input)\n    return w\n\nNow, I build and train the network.\n\nmodel2 = NeuralNetwork2()\noptimizer2 = torch.optim.SGD(model2.parameters(), lr=0.01, momentum=.9)\n\n\nepochs = 100000\nloss_values = np.zeros(100000)\nfor i in range(epochs):\n    l = loss2(x, y, t, xbdry, ybdry, tbdry, model2)\n    l.backward()\n    optimizer2.step()\n    optimizer2.zero_grad()\n    loss_values[i]=l\n    if i%1000==0:\n        print(\"Loss at epoch {}: {}\".format(i, l.item()))\n\nLoss at epoch 0: 0.89188551902771\nLoss at epoch 1000: 0.003854417707771063\nLoss at epoch 2000: 0.0035820240154862404\nLoss at epoch 3000: 0.003396423067897558\nLoss at epoch 4000: 0.0032313335686922073\nLoss at epoch 5000: 0.003081361996009946\nLoss at epoch 6000: 0.0029437472112476826\nLoss at epoch 7000: 0.0028166514821350574\nLoss at epoch 8000: 0.0026987330056726933\nLoss at epoch 9000: 0.002588952425867319\nLoss at epoch 10000: 0.002486460143700242\nLoss at epoch 11000: 0.0023905490525066853\nLoss at epoch 12000: 0.0023006070405244827\nLoss at epoch 13000: 0.0022161088418215513\nLoss at epoch 14000: 0.002136589726433158\nLoss at epoch 15000: 0.0020616455003619194\nLoss at epoch 16000: 0.0019909129478037357\nLoss at epoch 17000: 0.001924069132655859\nLoss at epoch 18000: 0.0018608317477628589\nLoss at epoch 19000: 0.0018009372288361192\nLoss at epoch 20000: 0.0017441592644900084\nLoss at epoch 21000: 0.001690280856564641\nLoss at epoch 22000: 0.0016391162062063813\nLoss at epoch 23000: 0.0015904909232631326\nLoss at epoch 24000: 0.0015442450530827045\nLoss at epoch 25000: 0.0015002338914200664\nLoss at epoch 26000: 0.0014583258889615536\nLoss at epoch 27000: 0.0014183972962200642\nLoss at epoch 28000: 0.0013803349575027823\nLoss at epoch 29000: 0.0013440358452498913\nLoss at epoch 30000: 0.0013093998422846198\nLoss at epoch 31000: 0.0012763398699462414\nLoss at epoch 32000: 0.0012447721092030406\nLoss at epoch 33000: 0.0012146164663136005\nLoss at epoch 34000: 0.0011857992503792048\nLoss at epoch 35000: 0.0011582525912672281\nLoss at epoch 36000: 0.0011319173499941826\nLoss at epoch 37000: 0.0011067262385040522\nLoss at epoch 38000: 0.001082623261027038\nLoss at epoch 39000: 0.001059556845575571\nLoss at epoch 40000: 0.0010374787962064147\nLoss at epoch 41000: 0.0010163375409319997\nLoss at epoch 42000: 0.000996090704575181\nLoss at epoch 43000: 0.0009766991715878248\nLoss at epoch 44000: 0.0009581181220710278\nLoss at epoch 45000: 0.0009403128060512245\nLoss at epoch 46000: 0.0009232479496859014\nLoss at epoch 47000: 0.0009068892104551196\nLoss at epoch 48000: 0.0008912016055546701\nLoss at epoch 49000: 0.0008761570206843317\nLoss at epoch 50000: 0.0008617275161668658\nLoss at epoch 51000: 0.0008478870149701834\nLoss at epoch 52000: 0.0008346029790118337\nLoss at epoch 53000: 0.0008218575385399163\nLoss at epoch 54000: 0.0008096238598227501\nLoss at epoch 55000: 0.0007978778448887169\nLoss at epoch 56000: 0.0007866068044677377\nLoss at epoch 57000: 0.0007757834391668439\nLoss at epoch 58000: 0.0007653866778127849\nLoss at epoch 59000: 0.0007554012699984014\nLoss at epoch 60000: 0.0007458093459717929\nLoss at epoch 61000: 0.0007365943747572601\nLoss at epoch 62000: 0.0007277417462319136\nLoss at epoch 63000: 0.0007192345801740885\nLoss at epoch 64000: 0.0007110605365596712\nLoss at epoch 65000: 0.0007032029679976404\nLoss at epoch 66000: 0.0006956501747481525\nLoss at epoch 67000: 0.0006883873138576746\nLoss at epoch 68000: 0.0006814048392698169\nLoss at epoch 69000: 0.0006746951257809997\nLoss at epoch 70000: 0.0006682390812784433\nLoss at epoch 71000: 0.0006620289059355855\nLoss at epoch 72000: 0.0006560610490851104\nLoss at epoch 73000: 0.0006503157783299685\nLoss at epoch 74000: 0.0006447912310250103\nLoss at epoch 75000: 0.0006394768133759499\nLoss at epoch 76000: 0.0006343616987578571\nLoss at epoch 77000: 0.0006294394261203706\nLoss at epoch 78000: 0.0006247067940421402\nLoss at epoch 79000: 0.0006201465730555356\nLoss at epoch 80000: 0.0006157630705274642\nLoss at epoch 81000: 0.0006115416181273758\nLoss at epoch 82000: 0.0006074761622585356\nLoss at epoch 83000: 0.0006035627448000014\nLoss at epoch 84000: 0.0005997983971610665\nLoss at epoch 85000: 0.0005961735732853413\nLoss at epoch 86000: 0.000592682627029717\nLoss at epoch 87000: 0.000589318631682545\nLoss at epoch 88000: 0.0005860833334736526\nLoss at epoch 89000: 0.0005829664296470582\nLoss at epoch 90000: 0.0005799678037874401\nLoss at epoch 91000: 0.0005770765710622072\nLoss at epoch 92000: 0.0005742927896790206\nLoss at epoch 93000: 0.0005716124433092773\nLoss at epoch 94000: 0.0005690286634489894\nLoss at epoch 95000: 0.0005665384232997894\nLoss at epoch 96000: 0.0005641438765451312\nLoss at epoch 97000: 0.0005618334980681539\nLoss at epoch 98000: 0.0005596067057922482\nLoss at epoch 99000: 0.0005574636743403971\n\n\nEvidently, the convergence to a solution is much more rapid (one needs far fewer epochs to obtain the same loss as in the more traditional neural network). Below are plots of the solution, as well as an animation. These look virtually identical to the results using the more traditional neural network.\n\nx_test, y_test, z_test, xbdry_test, ybdry_test, zbdry_test = SampleFromUnitBall(10000)\n\ndef plot_points2(time_index):\n    with torch.no_grad():\n        soln = network_value(x_test,y_test,torch.ones(10000).unsqueeze(1)*time_index/100,model2)\n    plt.scatter(x=x_test.detach(),y=y_test.detach(),c=soln.detach(), vmin=0, vmax=0.35)\n    plt.title(\"Solution at time index \" + str(time_index))\n    plt.xlabel(\"x-axis\")\n    plt.ylabel(\"y-axis\")\n    plt.show()\n\nplot_points2(0)\nplot_points2(3)\nplot_points2(9)\n\n\n\n\n\n\n\n\n\n\nInterestingly, projecting the solution into the future past \\(t=0.09\\) seems to yield a less accurate solution than the one I got from the neural network. But still, this isn’t bad and leads to some interesting theoretical questions as to the degree of polynomial required to get “future in time” predictions which are as accurate as the network.\n\nx_test, y_test, z_test, xbdry_test, ybdry_test, zbdry_test = SampleFromUnitBall(10000)\nfig, ax = plt.subplots()\nfor k in range(20):\n    with torch.no_grad():\n        soln = network_value(x_test,y_test,torch.ones(10000).unsqueeze(1)*k/100,model2)\n    ax.cla()\n    ax.scatter(x=x_test.detach(),y=y_test.detach(),c=soln.detach(), vmin=0, vmax=0.35)\n    display(fig)\n    clear_output(wait = True)\n    plt.pause(0.1)\n\n\n\n\nNow that I have all of this set up, it would be interesting to run the code against more complicated initial conditions. I leave this exercise to the reader. Next time, I think I’ll get back to something more like the Poisson equation, or perhaps work on solving PDEs which arise from variational problems. Overall, solving PDEs using neural networks or polynomial regression is proving productive and interesting!"
  },
  {
    "objectID": "Blog/Restricting the Dirac Fock energy to the Electron Subspace.html",
    "href": "Blog/Restricting the Dirac Fock energy to the Electron Subspace.html",
    "title": "Restricting the Dirac-Fock energy to the Electron Subspace",
    "section": "",
    "text": "The Dirac-Fock energy functional is the relativistic counterpart to the Hartree-Fock energy functional, and acts on four-component square integrable wave functions which arise from Slater determinants. This ensures the wave functions satisfy the particle statistics we expect from fermions. In normal atomic units, the Dirac-Fock energy \\(\\mathcal{E}\\) acts on an \\(N\\) electron wave function \\(\\psi\\in H^{1/2}(\\mathbb{R}^3;\\mathbb{C}^4)\\wedge ...\\wedge H^{1/2}(\\mathbb{R}^3;\\mathbb{C}^4)\\) via (Esteban and Séré 1999),\n\\[\\mathcal{E}(\\psi):=\\sum_{j=1}^N\\left((\\psi_j,D\\psi_j)-\\alpha Z\\left(\\psi_j,\\frac{1}{|x|}\\psi_j\\right)\\right)\\]\n\\[+\\frac{\\alpha}{2}\\int\\int_{\\mathbb{R}^3\\times\\mathbb{R}^3}\\frac{\\rho(x)\\rho(y)-\\text{tr}(R(x,y)R(y,x))}{|x-y|}dydx\\]\nHere, \\((\\cdot,\\cdot)\\) is the \\(L^2\\) inner product, \\(\\rho(x)\\) is the usual particle density and \\(R(x,y)=\\sum_{j=1}^N\\psi_j(x)\\otimes\\psi_j(y)^*\\) (the star denoting conjugate transpose). \\(\\alpha\\) is the fine structure constant, \\(Z\\) is the nuclear charge, and \\(D\\) is the Dirac operator in normal atomic units. Critical points of \\(\\mathcal{E}\\) solve the Dirac-Fock equations, which is an eigenvalue problem when an \\(L^2\\) constraint (typically norm one) is imposed."
  },
  {
    "objectID": "Blog/Restricting the Dirac Fock energy to the Electron Subspace.html#the-sobolev-space-hs",
    "href": "Blog/Restricting the Dirac Fock energy to the Electron Subspace.html#the-sobolev-space-hs",
    "title": "Restricting the Dirac-Fock energy to the Electron Subspace",
    "section": "The Sobolev Space \\(H^s\\)",
    "text": "The Sobolev Space \\(H^s\\)\nRecall that Sobolev spaces comprise of functions who’s derivative(s) exist in some weak sense. One of the purposes of considering such functions is to solve a PDE over a larger class of functions than the PDE’s domain. One can then try to recover appropriate regularity of the “weak” solution of the PDE. In this sense, one defines functions with \\(k\\) weak derivatives for some positive integer \\(k\\). It turns out you can generalize this notation to define fractional derivatives of functions. We reference (Evans 2010) and (Lieb and Loss 2001) for this discussion.\nThe space \\(H^s(\\mathbb{R}^n)\\) for any positive real number \\(s\\) consists of functions \\(f\\in L^2(\\mathbb{R}^n)\\) with the property that \\((1+|x|^s)\\widehat{f}\\in L^2(\\mathbb{R}^n)\\). The associated norm is,\n\\[||f||_{H^s(\\mathbb{R}^n)}:=||(1+|x|^s)\\widehat{f}||_2\\]\nIn the particular case of \\(H^{1/2}\\) this is sometimes equivalently characterized as,\n\\[||f||_{H^{1/2}(\\mathbb{R}^n)}^2:=||(1+|x|^2)^{1/2}|\\widehat{f}(x)|^2||_2^2\\]\nThis Fourier characterization of Sobolev spaces for integer \\(s\\) is equivalent to the more traditional definition of weakly differentiable functions. It can be very useful for both analytical as well as technical reasons. First, let me give a simple example of the former. Using the fourier characterization, one easily shows that if \\(f\\in H^s(\\mathbb{R}^n)\\) where \\(s&gt;\\frac{n}{2}\\), then in fact \\(f\\in L^\\infty(\\mathbb{R}^n)\\). This follows from the following computation,\n\\[|f(x)|=\\left|\\frac{1}{(2\\pi)^{n/2}}\\int_{\\mathbb{R}^n}e^{-ip\\cdot x}\\widehat{f}(p)dx\\right|\\leq\\frac{1}{(2\\pi)^{n/2}}\\int\\frac{1+|p|^s}{1+|p|^s}|\\widehat{f}(p)|dx \\]\n\\[\\leq\\frac{1}{(2\\pi)^{n/2}}||f||_{H^s(\\mathbb{R}^n)}^2\\left|\\left|\\frac{1}{1+|p|^s}\\right|\\right|_2^2\\]\nwhere the latter inequality follows by Holder’s inequality. Since \\(s&gt;\\frac{n}{2}\\), the right hand side is finite, as needed."
  },
  {
    "objectID": "Blog/Restricting the Dirac Fock energy to the Electron Subspace.html#fractional-differential-operators",
    "href": "Blog/Restricting the Dirac Fock energy to the Electron Subspace.html#fractional-differential-operators",
    "title": "Restricting the Dirac-Fock energy to the Electron Subspace",
    "section": "Fractional Differential Operators",
    "text": "Fractional Differential Operators\nThe fractional Sobolev space \\(H^{1/2}\\) arises naturally in relativistic quantum mechanics (Thaller 2011). The Klein-Gordon operator arises from quantizing the classical relativistic energy-momentum equation. The operator is,\n\\[\\sqrt{m^2c^4-c^2\\Delta}\\tag{1}\\]\nHow do we apply such an operator? Such fractional operators are defined using the Fourier transform.\n\\[\\sqrt{m^2c^4-c^2\\Delta}f(x):=\\left(\\sqrt{m^2c^4+c^2p^2}\\widehat{f}(p)\\right)^{\\vee}\\tag{2}\\]\nWhen solving variational problems in quantum mathematics, we study the energy of quantum systems and try to find the ground state of a quantum system. Alternatively, we seek critical points of quantum energy functionals. The operator (1) isn’t really what we analyze when studying a variational problem. In fact, we study the energy,\n\\[(f(x),\\sqrt{m^2c^4-c^2\\Delta}f(x))\\tag{3}\\]\nwhich makes sense when \\(f\\) is only in \\(H^{1/2}\\), whereas operating on \\(f\\) as in (2) requires \\(f\\in H^1\\) (since (2) needs to be in \\(L^2\\)). This amounts to a weakening of the weak differentiability requirements of a function we input into the energy (3). We say \\(f\\) is in the “form domain” of the Klein-Gordon operator."
  },
  {
    "objectID": "Blog/Restricting the Dirac Fock energy to the Electron Subspace.html#application-to-dirac",
    "href": "Blog/Restricting the Dirac Fock energy to the Electron Subspace.html#application-to-dirac",
    "title": "Restricting the Dirac-Fock energy to the Electron Subspace",
    "section": "Application to Dirac",
    "text": "Application to Dirac\nThe purpose of this post is not to give a complete exposition of the free Dirac operator; this is just a quick application of fractional operators to the operator I study every day. Let \\(D\\) denote the free Dirac operator with normalized units,\n\\[D_c:=-i\\pmb{\\alpha}\\cdot\\nabla+\\beta\\]\nwhere the three-vector \\(\\pmb{\\alpha}\\) has components \\(\\begin{pmatrix}0&\\pmb{\\sigma}_j\\\\\\pmb{\\sigma}_j&0\\end{pmatrix}\\), \\(\\pmb{\\sigma}_j\\) being the usual Pauli matrices. \\(\\beta:=\\begin{pmatrix}1&0\\\\0&-1\\end{pmatrix}\\). Thus, \\(D\\) operators on 4-spinors.\nThe Dirac operator, under the Foldy-Wouthuysen Transformation, takes the form,\n\\[\\begin{pmatrix}\\sqrt{1-\\Delta}&&0\\\\0&&-\\sqrt{1-\\Delta}\\end{pmatrix}\\tag{4}\\]\nThis makes it easy to see that the form domain of the Dirac operator is \\(H^{1/2}(\\mathbb{R}^3;\\mathbb{C}^4)\\). The Klein-Gordon equations arise in (4) from the fact that \\(|D|=\\sqrt{1-\\Delta}\\). In fact, if we consider the energy of a particle under the influence of Dirac, we may write,\n\\[(\\psi,H\\psi)_2=(\\psi^+,H\\psi^+)_2+(\\psi^-,H\\psi^-)_2\\]\nHere, \\(\\psi=\\psi^++\\psi^-\\) where \\(\\psi^+\\) lies in the positive spectral subspace induced by \\(D\\) and \\(\\psi^-\\) lies in the negative spectral subspace. Since \\((\\psi^+,H\\psi^+)=(\\psi^+,|H|\\psi^+)\\) and \\((\\psi^-,H\\psi^-)=-(\\psi^-,|H|\\psi^-)\\), we have,\n\\[(\\psi,H\\psi)_2=||\\psi^+||_{H^{1/2}}^2-||\\psi^-||_{H^{1/2}}^2\\]\nso that the energy may be written entirely in terms of the \\(H^{1/2}\\) norm of the positive and negative spectral components of \\(\\psi\\).\nIn the analysis of energy functionals based on Dirac (say, the Dirac-Fock energy), it turns out that the best way to prove the existence of critical points is to search for weak critical points; that is, the critical points are in the form domain \\(H^{1/2}\\) (Maria J. Esteban and Séré 1999). Afterwards, one may recover some regularity of the solutions and, in fact, the self-adjointness of the Dirac operator perturbed by the Coulomb may be used to prove that the solutions are in fact in \\(H^1\\) (M. J. Esteban and Sere 2001)."
  },
  {
    "objectID": "Blog/Restricting the Dirac Fock energy to the Electron Subspace.html#spectral-projections",
    "href": "Blog/Restricting the Dirac Fock energy to the Electron Subspace.html#spectral-projections",
    "title": "Restricting the Dirac-Fock energy to the Electron Subspace",
    "section": "Spectral Projections",
    "text": "Spectral Projections\nThe Dirac operator has purely absolutely continuous spectrum \\(\\left(-\\infty, -1\\right]\\cup \\left[1,\\infty\\right)\\). This is the primary source of numerical difficulties in dealing with functionals such as \\(\\mathcal{E}\\), which is not bounded from below. A possible means to correct this is as follows: \\(D\\) induces spectral projectors which take the form (Thaller 2011):\n\\[P^{\\pm}=\\frac{1}{2}\\left(1\\pm\\frac{D}{|D|}\\right)\\]\nThe Hilbert space \\(H^+:=P^+L^2(\\mathbb{R}^3;\\mathbb{C}^4)\\) denotes the positive spectral subspace, which corresponds to the “electron subspace.” In a sense, the orthogonal complement to this space denotes the space of positrons, and it is this mathematics that first led physicists to conjecture the existence of positrons. On \\(H^+\\), we have \\((\\psi,D\\psi)&gt;0\\) and in fact when \\(\\psi\\in H^{1/2}(\\mathbb{R}^3;\\mathbb{C}^4)\\), we have \\(||\\psi||_{H^{1/2}}^2=(\\psi,D\\psi)\\). Let \\(H:=P^+H^{1/2}(\\mathbb{R}^3;\\mathbb{C}^4)\\). By Kato’s inequality, one easily sees that \\(\\mathcal{E}\\) is bounded below when restricted to \\(H\\). Thus, since the electron subspace is presumably the physically interesting space to us, it is natural to minimize \\(\\mathcal{E}\\) on \\(H\\)."
  },
  {
    "objectID": "Blog/Restricting the Dirac Fock energy to the Electron Subspace.html#sketch-of-the-minimization-procedure",
    "href": "Blog/Restricting the Dirac Fock energy to the Electron Subspace.html#sketch-of-the-minimization-procedure",
    "title": "Restricting the Dirac-Fock energy to the Electron Subspace",
    "section": "Sketch of the Minimization Procedure",
    "text": "Sketch of the Minimization Procedure\nThe procedure here follows the same ideas as in (Lieb and Simon 1977). We’ve already mentioned that \\(\\mathcal{E}\\) is bounded below on \\(H\\), which follows from Kato’s inequality for \\(\\psi\\in H\\):\n\\[(\\psi,V\\psi)\\leq K(\\psi,D\\psi)\\]\n(\\(K=\\frac{\\pi}{4}+\\frac{1}{\\pi}\\)) and the fact that we have the estimate (Esteban and Séré 1999),\n\\[\\int\\int_{\\mathbb{R}^3\\times\\mathbb{R}^3}\\frac{\\rho(x)\\rho(y)}{|x-y|}dydx\\leq NK(\\psi, D\\psi)\\]\nIn particular, one has the estimate,\n\\[\\mathcal{E}(\\psi)\\geq \\left(1-\\alpha Z\\right)\\sum_{j=1}^N||\\psi_j||_{H^{1/2}}^2\\]\nwhich is positive with suitable restrictions on \\(Z\\). Hence, given a minimizing sequence \\(\\psi_n\\in H\\) with \\(||\\psi_n||_2\\leq 1\\) for \\(\\mathcal{E}\\), we have a weakly convergent subsequence \\(\\psi_n\\rightharpoonup \\psi\\) in \\(H\\). The goal is to show that \\(\\psi\\) is, in fact, the solution to the minimization problem and that \\(\\psi\\) is of unit norm. Relaxing a minimization problem in this sense (that is, assuming \\(||\\psi_n||_2\\leq 1\\)) is common in the literature, as one can often prove readily after getting a solution that the minimizing function does have the desired norm.\nLower-semicontinuity of \\(\\mathcal{E}\\) actually follows fairly readily, giving us a minimizer. Given \\(\\varphi_n\\rightharpoonup 0\\), we actually have (Coti Zelati and Nolasco 2019) \\((\\varphi_n,V\\varphi_n)\\rightarrow 0\\), so combining this, Kato’s inequality, and weak lower-semicontinuity of the norm yields weak lower-semicontinuity of \\(\\mathcal{E}\\).\nSimilar arguments to (Lieb and Simon 1977) yield the appropriate norm constraint on the solution, and a simple application of the Rayleigh-Ritz principle (see (Reed and Simon 1978) theorem 13.6) yield infinitely many eigenvalues below 1 of the Dirac-Fock mean field operator (when restricted to \\(H\\)).\nEvidently, extending (Lieb and Simon 1977) to the Dirac-Fock case when restricted to \\(H\\) is not terribly difficult. More challenging is the existence of critical points of \\(\\mathcal{E}\\) with no restriction of \\(\\mathcal{E}\\) to \\(H\\), in which case one needs to deal with the unboundedness of \\(\\mathcal{E}\\). This is solved given suitable restrictions on \\(N\\) and \\(Z\\) in (Esteban and Séré 1999). In any case, Kato’s inequality allows one to very nicely extend the classical results of Lieb and Simon to the relativistic theory."
  }
]