[
  {
    "objectID": "Blog.html",
    "href": "Blog.html",
    "title": "Blog",
    "section": "",
    "text": "I post blog entries discussing a variety of mathematical topics, as well as some personal anecdotes about my experiences in academia. I try to post a variety of topics, including those related to my research as well as some slightly off-topic mathematical interests. Please note that I’m the only one who proofreads the material I post; if you notice any errors, please let me know and I will be happy to correct them.\n\n\n\n\n\n\n\n\n  \n\n\n\n\nNeural Poisson Solver\n\n\n\n\n\n\n\nPartial Differential Equations\n\n\nMachine Learning\n\n\n\n\nSolving a simple PDE using a neural network\n\n\n\n\n\n\nSep 8, 2023\n\n\nZach Wagner\n\n\n\n\n\n\n  \n\n\n\n\nFractional Derivatives\n\n\n\n\n\n\n\nOperator Theory\n\n\nFunctional Analysis\n\n\nPartial Differential Equations\n\n\n\n\nA quick introduction to the theory and some applications of fractional derivatives.\n\n\n\n\n\n\nAug 3, 2023\n\n\nZach Wagner\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "Teaching.html",
    "href": "Teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Math 6A, Summer Session A 2021 (Remote Instruction)"
  },
  {
    "objectID": "Teaching.html#courses-as-instructor-of-record",
    "href": "Teaching.html#courses-as-instructor-of-record",
    "title": "Teaching",
    "section": "",
    "text": "Math 6A, Summer Session A 2021 (Remote Instruction)"
  },
  {
    "objectID": "Teaching.html#courses-as-a-teaching-assistant",
    "href": "Teaching.html#courses-as-a-teaching-assistant",
    "title": "Teaching",
    "section": "Courses as a Teaching Assistant",
    "text": "Courses as a Teaching Assistant\n\nMath 6A, Winter 2022\nMath 6B, Fall 2021\nMath 117, Summer Session B 2021 (Remote Instruction)\nMath 4B, Spring 2021 (Remote Instruction)\nMath 118B, Winter 2021 (Remote Instruction)\nMath 118A, Fall 2020 (Remote Instruction)\nMath 6B, Summer Session B 2020 (Remote Instruction)\nMath 3A, Spring 2020 (Remote Instruction)\nMath 6B, Winter 2020\nMath 34A, Fall 2019"
  },
  {
    "objectID": "Research.html",
    "href": "Research.html",
    "title": "Research",
    "section": "",
    "text": "Here’s my CV (Updated July 2023)\nI’m working with the applied mathematics group at UCSB, under the supervision of Carlos Garcia-Cervera. My general areas of research are:\n\nRelativistic Quantum Mechanics\nVariational Problems Pertaining to Energy Minimization of Quantum Systems\nFunctional Analysis\nPartial Differential Equations\nNumerical Methods in Quantum Mathematics\n\nI’ll post publications/preprints here when they become available."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome!",
    "section": "",
    "text": "I am a fifth year mathematics Ph.D. candidate at the University of California, Santa Barbara.\nMy office is in South Hall, Room 6432K (in the grad tower).\nI maintain my blog on this site, as well as post research updates.\nemail"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site"
  },
  {
    "objectID": "Blog/Fractional Derivatives.html",
    "href": "Blog/Fractional Derivatives.html",
    "title": "Fractional Derivatives",
    "section": "",
    "text": "This is meant to be a very quick exposition on fractional derivatives, with particular emphasis paid to functions in \\(H^{1/2}\\). When I introduce my area of research to colleagues, I inevitably discuss four-spinors \\(\\psi\\in H^{1/2}(\\mathbb{R}^3;\\mathbb{C}^4)\\). This usually leads to an impromptu discussion of \\(H^{1/2}\\) and why we care about it. I’ll attempt to give a brief overview of these ideas here."
  },
  {
    "objectID": "Blog/Fractional Derivatives.html#the-sobolev-space-hs",
    "href": "Blog/Fractional Derivatives.html#the-sobolev-space-hs",
    "title": "Fractional Derivatives",
    "section": "The Sobolev Space \\(H^s\\)",
    "text": "The Sobolev Space \\(H^s\\)\nRecall that Sobolev spaces comprise of functions who’s derivative(s) exist in some weak sense. One of the purposes of considering such functions is to solve a PDE over a larger class of functions than the PDE’s domain. One can then try to recover appropriate regularity of the “weak” solution of the PDE. In this sense, one defines functions with \\(k\\) weak derivatives for some positive integer \\(k\\). It turns out you can generalize this notation to define fractional derivatives of functions. We reference (Evans 2010) and (Lieb and Loss 2001) for this discussion.\nThe space \\(H^s(\\mathbb{R}^n)\\) for any positive real number \\(s\\) consists of functions \\(f\\in L^2(\\mathbb{R}^n)\\) with the property that \\((1+|x|^s)\\widehat{f}\\in L^2(\\mathbb{R}^n)\\). The associated norm is,\n\\[||f||_{H^s(\\mathbb{R}^n)}:=||(1+|x|^s)\\widehat{f}||_2\\]\nIn the particular case of \\(H^{1/2}\\) this is sometimes equivalently characterized as,\n\\[||f||_{H^{1/2}(\\mathbb{R}^n)}^2:=||(1+|x|^2)^{1/2}|\\widehat{f}(x)|^2||_2^2\\]\nThis Fourier characterization of Sobolev spaces for integer \\(s\\) is equivalent to the more traditional definition of weakly differentiable functions. It can be very useful for both analytical as well as technical reasons. First, let me give a simple example of the former. Using the fourier characterization, one easily shows that if \\(f\\in H^s(\\mathbb{R}^n)\\) where \\(s&gt;\\frac{n}{2}\\), then in fact \\(f\\in L^\\infty(\\mathbb{R}^n)\\). This follows from the following computation,\n\\[|f(x)|=\\left|\\frac{1}{(2\\pi)^{n/2}}\\int_{\\mathbb{R}^n}e^{-ip\\cdot x}\\widehat{f}(p)dx\\right|\\leq\\frac{1}{(2\\pi)^{n/2}}\\int\\frac{1+|p|^s}{1+|p|^s}|\\widehat{f}(p)|dx \\]\n\\[\\leq\\frac{1}{(2\\pi)^{n/2}}||f||_{H^s(\\mathbb{R}^n)}^2\\left|\\left|\\frac{1}{1+|p|^s}\\right|\\right|_2^2\\]\nwhere the latter inequality follows by Holder’s inequality. Since \\(s&gt;\\frac{n}{2}\\), the right hand side is finite, as needed."
  },
  {
    "objectID": "Blog/Fractional Derivatives.html#fractional-differential-operators",
    "href": "Blog/Fractional Derivatives.html#fractional-differential-operators",
    "title": "Fractional Derivatives",
    "section": "Fractional Differential Operators",
    "text": "Fractional Differential Operators\nThe fractional Sobolev space \\(H^{1/2}\\) arises naturally in relativistic quantum mechanics (Thaller 2011). The Klein-Gordon operator arises from quantizing the classical relativistic energy-momentum equation. The operator is,\n\\[\\sqrt{m^2c^4-c^2\\Delta}\\tag{1}\\]\nHow do we apply such an operator? Such fractional operators are defined using the Fourier transform.\n\\[\\sqrt{m^2c^4-c^2\\Delta}f(x):=\\left(\\sqrt{m^2c^4+c^2p^2}\\widehat{f}(p)\\right)^{\\vee}\\tag{2}\\]\nWhen solving variational problems in quantum mathematics, we study the energy of quantum systems and try to find the ground state of a quantum system. Alternatively, we seek critical points of quantum energy functionals. The operator (1) isn’t really what we analyze when studying a variational problem. In fact, we study the energy,\n\\[(f(x),\\sqrt{m^2c^4-c^2\\Delta}f(x))\\tag{3}\\]\nwhich makes sense when \\(f\\) is only in \\(H^{1/2}\\), whereas operating on \\(f\\) as in (2) requires \\(f\\in H^1\\) (since (2) needs to be in \\(L^2\\)). This amounts to a weakening of the weak differentiability requirements of a function we input into the energy (3). We say \\(f\\) is in the “form domain” of the Klein-Gordon operator."
  },
  {
    "objectID": "Blog/Fractional Derivatives.html#application-to-dirac",
    "href": "Blog/Fractional Derivatives.html#application-to-dirac",
    "title": "Fractional Derivatives",
    "section": "Application to Dirac",
    "text": "Application to Dirac\nThe purpose of this post is not to give a complete exposition of the free Dirac operator; this is just a quick application of fractional operators to the operator I study every day. Let \\(D\\) denote the free Dirac operator with normalized units,\n\\[D_c:=-i\\pmb{\\alpha}\\cdot\\nabla+\\beta\\]\nwhere the three-vector \\(\\pmb{\\alpha}\\) has components \\(\\begin{pmatrix}0&\\pmb{\\sigma}_j\\\\\\pmb{\\sigma}_j&0\\end{pmatrix}\\), \\(\\pmb{\\sigma}_j\\) being the usual Pauli matrices. \\(\\beta:=\\begin{pmatrix}1&0\\\\0&-1\\end{pmatrix}\\). Thus, \\(D\\) operators on 4-spinors.\nThe Dirac operator, under the Foldy-Wouthuysen Transformation, takes the form,\n\\[\\begin{pmatrix}\\sqrt{1-\\Delta}&&0\\\\0&&-\\sqrt{1-\\Delta}\\end{pmatrix}\\tag{4}\\]\nThis makes it easy to see that the form domain of the Dirac operator is \\(H^{1/2}(\\mathbb{R}^3;\\mathbb{C}^4)\\). The Klein-Gordon equations arise in (4) from the fact that \\(|D|=\\sqrt{1-\\Delta}\\). In fact, if we consider the energy of a particle under the influence of Dirac, we may write,\n\\[(\\psi,H\\psi)_2=(\\psi^+,H\\psi^+)_2+(\\psi^-,H\\psi^-)_2\\]\nHere, \\(\\psi=\\psi^++\\psi^-\\) where \\(\\psi^+\\) lies in the positive spectral subspace induced by \\(D\\) and \\(\\psi^-\\) lies in the negative spectral subspace. Since \\((\\psi^+,H\\psi^+)=(\\psi^+,|H|\\psi^+)\\) and \\((\\psi^-,H\\psi^-)=-(\\psi^-,|H|\\psi^-)\\), we have,\n\\[(\\psi,H\\psi)_2=||\\psi^+||_{H^{1/2}}^2-||\\psi^-||_{H^{1/2}}^2\\]\nso that the energy may be written entirely in terms of the \\(H^{1/2}\\) norm of the positive and negative spectral components of \\(\\psi\\).\nIn the analysis of energy functionals based on Dirac (say, the Dirac-Fock energy), it turns out that the best way to prove the existence of critical points is to search for weak critical points; that is, the critical points are in the form domain \\(H^{1/2}\\) (Maria J. Esteban and Séré 1999). Afterwards, one may recover some regularity of the solutions and, in fact, the self-adjointness of the Dirac operator perturbed by the Coulomb may be used to prove that the solutions are in fact in \\(H^1\\) (M. J. Esteban and Sere 2001)."
  },
  {
    "objectID": "Blog/Neural Poisson Solver.html",
    "href": "Blog/Neural Poisson Solver.html",
    "title": "Neural Poisson Solver",
    "section": "",
    "text": "This is the first of a series of posts discussing a side project I’m working on. Long term, I’m interested in solving the PDEs I’m studying in my research using deep neural networks. This is a natural approach, but having not implemented such a program before, I’m starting with some easier problems to get myself familiar with the implementation of such a solver using Pytorch. Most of my experience up until recently has been with Tensorflow (I have a Tensorflow Developer certification from Coursera). So far, I’ve enjoyed Pytorch and I’ve found it much more intuitive than Tensorflow."
  },
  {
    "objectID": "Blog/Neural Poisson Solver.html#poisson-equation",
    "href": "Blog/Neural Poisson Solver.html#poisson-equation",
    "title": "Neural Poisson Solver",
    "section": "Poisson Equation",
    "text": "Poisson Equation\nThe first toy problem I present here is the Poisson Equation with Dirichlet boundary condition on the unit disk. I’ve chosen a very simple relationship for \\(\\Delta u\\) and the boundary condition to quickly show that the neural network I train differs only slightly from the analytical solution.\nThe problem I solve is this,\n\\[\\Delta u(x,y)=1\\quad (x,y)\\in B_1(0)\\] \\[u(x,y)=0\\quad (x,y)\\in\\partial B_1(0)\\]\nin \\(\\mathbb{R}^2\\). The analytical solution to this problem is easily seen to be \\(u(x,y)=\\frac{1}{4}(x^2+y^2-1)\\).\n\nimport torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\n\nWe will use a very simple neural network.\n\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.sequential_model = nn.Sequential(\n            nn.Linear(2, 8),\n            nn.Sigmoid(),\n            nn.Linear(8, 1)\n        )\n\n    def forward(self, x):\n        return self.sequential_model(x)\n\nHere, we define a function which samples the necessary data to train the network (and test the network later).\n\ndef SampleFromUnitDisk(points):\n    d = torch.distributions.Uniform(-1,1)\n\n    x = torch.Tensor(points,1)\n    y = torch.Tensor(points,1)\n    j=0\n\n    while j&lt;points:\n        x_temp = d.sample()\n        y_temp = d.sample()\n        if x_temp**2+y_temp**2&lt;1:\n            x[j,0]=x_temp\n            y[j,0]=y_temp\n            j+=1\n\n    xbdry = torch.Tensor(points,1)\n    ybdry = torch.Tensor(points,1)\n    j=0\n\n    #Vary the sign of the y coordinate, for otherwise we'd only have positive y values.\n\n    for j in range(points):\n        x_temp = d.sample()\n        xbdry[j,0]=x_temp\n        if j%2==0:\n            ybdry[j,0]=math.sqrt(1-x_temp**2)\n        else:\n            ybdry[j,0]=-math.sqrt(1-x_temp**2)\n\n    return x, y, xbdry, ybdry\n\nNow, we generate the training data.\n\nx, y, xbdry, ybdry = SampleFromUnitDisk(10000)\n\nLet’s discuss the loss function. Since we’ve descritized the domain, we are going to use a discrete mean-squared error function to compute the loss. In our case, we want to minimize the following,\n\\[L(x_{\\text{int}},y_\\text{int}, x_{\\text{bdry}},y_{\\text{bdry}}):=\\frac{1}{N_{\\text{int}}}\\sum_{j=1}^{N_{\\text{int}}}|\\Delta u(x^{(j)}_{\\text{int}},y^{(j)}_{\\text{int}})-1|^2+\\frac{1}{N_{\\text{bdry}}}\\sum_{j=1}^{N_{\\text{bdry}}}|u(x^{(j)}_{\\text{bdry}},y^{(j)}_{\\text{bdry}})|^2\\]\nThe first term comes from the fact that \\(\\Delta u(x,y)=1\\) on the interior, while \\(u=0\\) identically on the boundary. To implement this, we define the following.\n\ndef loss(x, y, xbdry, ybdry, network):\n    x.requires_grad = True\n    y.requires_grad = True\n    temp_input = torch.cat((x,y),1)\n    z=network(temp_input)\n    zbdry = network(torch.cat((xbdry, ybdry),1))\n\n    dz_dx = torch.autograd.grad(z.sum(), x, create_graph = True)[0]\n    ddz_ddx = torch.autograd.grad(dz_dx.sum(), x, create_graph = True)[0]\n    dz_dy = torch.autograd.grad(z.sum(), y, create_graph = True)[0] \n    ddz_ddy = torch.autograd.grad(dz_dy.sum(), y, create_graph = True)[0]\n\n    return torch.mean((ddz_ddx+ddz_ddy-1)**2)+torch.mean((zbdry-torch.zeros(xbdry.size(0)))**2)\n\nOkay now let’s create our network and train it! We’ll use 2000 epochs.\n\nmodel = NeuralNetwork()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=.9)\n\nepochs = 2000\nloss_values = np.zeros(2000)\nfor i in range(epochs):\n    l = loss(x, y, xbdry, ybdry, model)\n    l.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    loss_values[i]=l\n    if i%100==0:\n        print(\"Loss at epoch {}: {}\".format(i, l.item()))\n\nLoss at epoch 0: 1.0297363996505737\nLoss at epoch 100: 0.7696019411087036\nLoss at epoch 200: 0.10838701575994492\nLoss at epoch 300: 0.040865253657102585\nLoss at epoch 400: 0.02691115066409111\nLoss at epoch 500: 0.02078372612595558\nLoss at epoch 600: 0.017054535448551178\nLoss at epoch 700: 0.0143966656178236\nLoss at epoch 800: 0.012377198785543442\nLoss at epoch 900: 0.010792599990963936\nLoss at epoch 1000: 0.00952119193971157\nLoss at epoch 1100: 0.008482505567371845\nLoss at epoch 1200: 0.007620864547789097\nLoss at epoch 1300: 0.006896625738590956\nLoss at epoch 1400: 0.006280942354351282\nLoss at epoch 1500: 0.005752396769821644\nLoss at epoch 1600: 0.005294801667332649\nLoss at epoch 1700: 0.004895701073110104\nLoss at epoch 1800: 0.00454533938318491\nLoss at epoch 1900: 0.004236005246639252\n\n\nCool, so it seems like we’ve decreased the loss significantly. This is enough for our toy example. Here’s the loss decrease over the course of training:\n\nx_axis = np.linspace(0,2000,2000)[:,None]\nplt.figure(figsize=(5,3))\nplt.plot(x_axis,loss_values,'red', label='loss')\nplt.xlabel('Iteration')\nplt.ylabel('Loss')\nplt.legend()\n\n&lt;matplotlib.legend.Legend at 0x48cc0d390&gt;\n\n\n\n\n\nNow, let’s simulate a sup-norm test agains the analytic solution, using a test set of data.\n\nx_test, y_test, xbdry_test, ybdry_test = SampleFromUnitDisk(10000)\n\nwith torch.no_grad():\n    z = model(torch.cat((x_test,y_test),1))-(1/4*(x_test**2+y_test**2-1))\n    print(\"Interior sup-norm error: {}\".format(torch.max(abs(z)).item()))\n\nwith torch.no_grad():\n    z = model(torch.cat((xbdry_test,ybdry_test),1))-(1/4*(xbdry_test**2+ybdry_test**2-1))\n    print(\"Boundary sup-norm error: {}\".format(torch.max(abs(z)).item()))\n\nInterior sup-norm error: 0.04881325364112854\nBoundary sup-norm error: 0.04888451099395752\n\n\nOf course, we want to do better, but this is pretty decent for this toy example.\nCool, so we have a baseline implementation for solving PDEs using neural networks. This was of course extremely simple. I started messing with more complicated PDEs (quasilinear, nonlinear) and ran into challenges in both implementation and validation. There are some theoretical questions on how to deal with non-uniqueness of solutions and how to give an “ansatz” when initializaing training. Some of this will probably be the subject of my next post."
  }
]